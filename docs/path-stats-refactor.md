# è·¯å¾„ç»Ÿè®¡ä¸ç®¡ç†é‡æ„æŠ€æœ¯æ–¹æ¡ˆ

## 1. èƒŒæ™¯ä¸ç›®æ ‡

- ç°æœ‰ `/paths` æ¥å£åœ¨è¯»å– PathCollector Durable Object (DO) æ•°æ®æ—¶éœ€è¦ fan-out å¤šä¸ª DO å®ä¾‹ï¼Œå­˜åœ¨è¶…æ—¶å’Œå»¶è¿ŸæŠ–åŠ¨ã€‚
- ç»Ÿè®¡é“¾è·¯é‡‡é›†ã€èšåˆã€è¿”å›å…¨éƒ¨åŒæ­¥å®Œæˆï¼Œéš¾ä»¥æ”¯æ’‘é«˜å¹¶å‘åŠå¤§æ•°æ®é‡åœºæ™¯ã€‚
- æ•°æ®å±‚æ··ç”¨ KV ä¸ DO å†…å­˜ç»“æ„ï¼Œç¼ºä¹å†·çƒ­åˆ†å±‚ï¼Œæ— æ³•æ»¡è¶³é•¿æœŸåˆ†æéœ€æ±‚ã€‚

**é‡æ„ç›®æ ‡**

1. å°†è·¯å¾„è®¿é—®é‡‡é›†ä¸èšåˆè§£è€¦ï¼Œé™ä½æ¥å£å»¶è¿Ÿå¹¶æé«˜å¯æ‰©å±•æ€§ã€‚
2. å»ºç«‹äº‹ä»¶é©±åŠ¨ + å¼‚æ­¥èšåˆ + å¤šå±‚ç¼“å­˜çš„ Cloudflare åŸç”Ÿæ¶æ„ã€‚
3. å¼•å…¥æ ‡å‡†åŒ–ç›‘æ§ä¸å‘Šè­¦ï¼Œç¡®ä¿æ•°æ®å»¶è¿Ÿå’Œå¤„ç†é‡å¯è§‚æµ‹ã€‚
4. ä¸ºåç»­å¤šç»´ç»Ÿè®¡ã€å®æ—¶çœ‹æ¿æä¾›å¯æ‰©å±•åŸºç¡€ã€‚

## 2. ç°çŠ¶ä¸»è¦é—®é¢˜

| ç»´åº¦ | é—®é¢˜æè¿° | å½±å“ |
|------|---------|------|
| æ¶æ„è€¦åˆ | PathCollector å³é‡‡é›†åˆèšåˆï¼Œæ¥å£å¿…é¡»åŒæ­¥è¯»å– DO | è¯·æ±‚æ˜“è¶…æ—¶ï¼Œæ‰©å±•æ€§å·® |
| æ•°æ®å±‚ | DO å†…å­˜ + KV æ··ç”¨ï¼Œç¼ºå°‘æŒä¹…åŒ–æ˜ç»† | éš¾ä»¥åšå†å²åˆ†æï¼Œå¯é æ€§ä¸è¶³ |
| ç¼“å­˜ç­–ç•¥ | æ— åˆ†å±‚ç¼“å­˜ï¼Œçƒ­é—¨æ¥å£æ¯æ¬¡éƒ½å›æº DO | å»¶è¿Ÿé«˜ã€æˆæœ¬é«˜ |
| ç›‘æ§è¿ç»´ | ç¼ºå°‘é˜Ÿåˆ—ç§¯å‹/èšåˆè€—æ—¶æŒ‡æ ‡ | æ•…éšœæ— æ³•å¿«é€Ÿå®šä½ |

## 3. è®¾è®¡åŸåˆ™

- **äº‹ä»¶é©±åŠ¨**ï¼šé‡‡é›†â†’é˜Ÿåˆ—â†’èšåˆâ†’ç¼“å­˜ï¼Œé¿å…è¯·æ±‚é“¾è·¯é˜»å¡ã€‚
- **å†·çƒ­åˆ†å±‚**ï¼šçƒ­æ•°æ® Cacheï¼Œæ¸©æ•°æ® KVï¼Œå†·æ•°æ® D1ï¼Œå½’æ¡£ R2ã€‚
- **å¯è§‚æµ‹æ€§**ï¼šå…¨é“¾è·¯åŸ‹ç‚¹ã€æŒ‡æ ‡ã€å‘Šè­¦ã€‚
- **å¯æ¸è¿›æ¼”è¿›**ï¼šæ‹†åˆ†ä¸ºå¤šé˜¶æ®µè¿­ä»£ï¼Œé€æ­¥æ›¿æ¢ç°æœ‰é€»è¾‘ã€‚

## 4. æ€»ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Worker  â”‚ --> â”‚ Durable DO â”‚ --> â”‚ Workers Queueâ”‚ --> â”‚Aggregatorâ”‚
â”‚(è¯·æ±‚é‡‡é›†) â”‚     â”‚(è½»é‡é‡‡é›†)  â”‚     â”‚             â”‚     â”‚  Worker  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                                                              â”‚
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚                      æ•°æ®å±‚                         â”‚
                                    â”‚  D1 (æ˜ç»†/èšåˆ)   KV(å¿«ç…§)   Workers Cache   R2å½’æ¡£ â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚              â”‚              â”‚
                                             â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
                                             â”‚ /paths   â”‚  â”‚ /stats   â”‚   â”‚ å‰ç«¯SWR â”‚
                                             â”‚ APIå±‚    â”‚  â”‚ å…¶ä»–æ¥å£ â”‚   â”‚ åˆ·æ–°æŒ‰é’®â”‚
                                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 5. è¯¦ç»†è®¾è®¡

### 5.1 æ•°æ®é‡‡é›†å±‚

- **ä½ç½®**ï¼šç°æœ‰ Worker ä¸­çš„ç¼“å­˜ä¸­é—´ä»¶ã€è·¯å¾„ç»Ÿè®¡ä¸­é—´ä»¶ã€‚
- **ç­–ç•¥**ï¼š
  - é‡‡é›†å­—æ®µï¼š`path`ã€`method`ã€`status`ã€`responseTime`ã€`clientIP`ã€`timestamp`ã€`requestId`ã€‚
  - å†™å…¥é˜Ÿåˆ—å‰åšé™æµ/é‡‡æ ·ï¼Œé˜²æ­¢æ¶æ„æµé‡æ”¾å¤§ï¼›å¿…è¦æ—¶å¯è®¾ç½® sampling ratioã€‚
  - å®ç°å¹‚ç­‰ IDï¼ˆä¾‹å¦‚ `${timestamp}-${hash(clientIP + path + requestId).slice(0, 8)}`ï¼‰ï¼Œé¿å…é‡å¤æ¶ˆè´¹ã€‚
    - å¹‚ç­‰ ID è¦æ±‚ï¼šå›ºå®šé•¿åº¦ï¼ˆâ‰¤64 å­—ç¬¦ï¼‰ã€æ—¶é—´å‰ç¼€ä¾¿äºè°ƒè¯•ã€åŒ…å«è¯·æ±‚å”¯ä¸€æ€§æ ‡è¯†ã€‚
    - Worker ä¸­å·²å…·å¤‡ `crypto.subtle.digest` ä¸ `requestId` ç”Ÿæˆèƒ½åŠ›ï¼Œå¯ç›´æ¥å®ç°ã€‚
- **æŠ€æœ¯**ï¼š**Worker ç›´æ¥å†™å…¥ Workers Queuesï¼ˆGAï¼‰**ï¼Œè·³è¿‡ PathCollector DO è½¬å‘ï¼Œé™ä½å»¶è¿Ÿã€‚
  - Phase 1 ä¿ç•™æ—§ DO èšåˆé€»è¾‘ä½œä¸ºå…œåº•è¯»è·¯å¾„ï¼Œé€æ­¥ä¸‹çº¿ï¼›é¿å…åœ¨è¿ç§»æœŸé—´å¼•å…¥é¢å¤– DO fetch å¾€è¿”ã€‚
  - åŒå†™æœŸé—´é€šè¿‡å¹‚ç­‰ ID åœ¨ Aggregator ç«¯å»é‡ï¼Œç¡®ä¿ä¸é‡å¤è®¡æ•°ã€‚

### 5.2 é˜Ÿåˆ—ä¸èšåˆ

- **Queue Consumer / Aggregator Worker**ï¼š
  - æ¶ˆè´¹ç®¡é“ï¼šWorkers Queuesï¼Œå•æ¡æ¶ˆæ¯ â‰¤128KBï¼Œæ”¯æŒæœ€å¤š 20 æ¬¡é‡è¯•ä¸æ­»ä¿¡ã€‚
  - æ‰¹é‡æ¶ˆè´¹äº‹ä»¶ï¼ˆå»ºè®®æ¯æ‰¹ 50~100 æ¡ï¼‰ï¼Œæ‰¹å†…å¤„ç†å¯ä¸²è¡Œæˆ–æœ‰é™åº¦å¹¶è¡Œã€‚
  - **åœ¨å†…å­˜ä¸­å®Œæˆç»Ÿè®¡åˆå¹¶**ï¼š
    - è¯»å–å½“å‰å°æ—¶æ¡¶çš„ `tdigest`ã€`hll_ip` å­—æ®µï¼ˆBLOBï¼‰ã€‚
    - åœ¨ Worker ä¸­ååºåˆ—åŒ–ï¼Œä½¿ç”¨ t-digest/HLL åº“ï¼ˆå¦‚ `tdigest`ã€`hyperloglog`ï¼‰å®Œæˆå¢é‡åˆå¹¶ã€‚
    - åºåˆ—åŒ–åå†™å› D1ï¼Œ**ä¸ä¾èµ–è‡ªå®šä¹‰ SQL å‡½æ•°**ï¼ˆD1 ç›®å‰ä¸æ”¯æŒ UDFï¼‰ã€‚
  - æ‰¹é‡å†™å…¥ D1ï¼š
    - æ˜ç»†è¡¨ï¼š`traffic_events`ï¼ˆå•è¡¨ + `event_date` å­—æ®µåˆ’åˆ†ï¼Œ**éœ€è®¾ç½®ä¿ç•™ç­–ç•¥**ï¼Œè§ä¸‹æ–‡ï¼‰ã€‚
    - èšåˆè¡¨ï¼š`path_stats_{granularity}`ï¼ˆå°æ—¶/æ—¥ç²’åº¦ï¼Œç´¯è®¡ PVã€é”™è¯¯æ•°ã€æ—¶å»¶æ‘˜è¦ï¼‰ã€‚
  - æ›´æ–° KV å¿«ç…§ï¼š
    - ä¿å­˜ Top N çƒ­é—¨è·¯å¾„ã€æ€»è¯·æ±‚æ•°ã€æœ€æ–°èšåˆæ—¶é—´ã€ç‰ˆæœ¬å·ç­‰ã€‚
  - åŒæ­¥é‡è¦æŒ‡æ ‡åˆ° Workers Analytics Engine ä»¥æ”¯æŒå¤æ‚æŸ¥è¯¢ã€‚
  - å¤„ç†å®Œæˆåè®°å½•æ¶ˆè´¹æ¬¡æ•°ã€æœ€å¤§å»¶è¿Ÿç­‰æŒ‡æ ‡ã€‚

### 5.3 æ•°æ®å­˜å‚¨åˆ†å±‚ä¸ä¿ç•™ç­–ç•¥

| å±‚çº§ | å­˜å‚¨ | ä½œç”¨ | ç¤ºæ„å†…å®¹ | ä¿ç•™æœŸé™ |
|------|------|------|----------|----------|
| çƒ­ | Workers Cache | æ¯«ç§’çº§å“åº”ï¼›ä¸šåŠ¡å±‚å®ç° SWR | `/paths` å¿«ç…§è§†å›¾ | 5 åˆ†é’Ÿ |
| æ¸© | KV | ä¿å­˜å¿«ç…§ã€ç‰ˆæœ¬å·ã€Top N | `paths:snapshot:{version}` | 72 å°æ—¶ï¼ˆ3 å¤©ï¼‰ |
| å†· | D1 | æ˜ç»†äº‹ä»¶ & èšåˆæŒ‡æ ‡ | `traffic_events`, `path_stats_hourly` | æ˜ç»† **3 å¤©**ï¼Œèšåˆ **90 å¤©** |
| å½’æ¡£ | R2 | é•¿æœŸå½’æ¡£ã€ä½æˆæœ¬å­˜å‚¨ | `events-archive/2025-01.parquet` | æ— é™æœŸæˆ–æŒ‰åˆè§„è¦æ±‚ |

#### D1 å®¹é‡ç®¡ç†ç­–ç•¥

**é—®é¢˜**ï¼šæŒ‰ 100 ä¸‡è¯·æ±‚/æ—¥ã€æ¯æ¡äº‹ä»¶ 150 B è®¡ç®—ï¼Œå•åº“ 1 GB ä¸Šé™å°†åœ¨ **6~7 å¤©å†…å¡«æ»¡**ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. **æ˜ç»†äº‹ä»¶å¼ºåˆ¶ä¿ç•™ 3 å¤©**ï¼š
   - æ¯æ—¥è¿è¡Œæ¸…ç†ä»»åŠ¡ï¼ˆCron Triggerï¼‰ï¼Œåˆ é™¤ `event_date < CURRENT_DATE - 3` çš„è®°å½•ã€‚
   - åˆ é™¤å‰å…ˆå½’æ¡£è‡³ R2ï¼ˆè§ä¸‹æ–‡ï¼‰ï¼Œç¡®ä¿æ•°æ®å¯è¿½æº¯ã€‚
   
2. **èšåˆæ•°æ®ä¿ç•™ 90 å¤©**ï¼š
   - èšåˆè¡¨å ç”¨ç©ºé—´è¿œå°äºæ˜ç»†ï¼ˆçº¦ 1/1000ï¼‰ï¼Œå¯å®‰å…¨ä¿ç•™æ›´é•¿æ—¶é—´ã€‚
   - è¶…è¿‡ 90 å¤©çš„èšåˆæ•°æ®å¯é€‰æ‹©æ€§å½’æ¡£è‡³ R2 æˆ–ç›´æ¥åˆ é™¤ã€‚

3. **R2 å½’æ¡£æµç¨‹**ï¼ˆæ¯æ—¥è‡ªåŠ¨è§¦å‘ï¼‰ï¼š
   ```
   1. æŸ¥è¯¢æ˜¨æ—¥æ˜ç»†ï¼šSELECT * FROM traffic_events WHERE event_date = '2025-10-13'
   2. è½¬æ¢ä¸º Parquet æ ¼å¼ï¼ˆä½¿ç”¨ apache-arrow åº“ï¼‰
   3. ä¸Šä¼ è‡³ R2ï¼ševents-archive/2025/10/2025-10-13.parquet
   4. éªŒè¯ä¸Šä¼ æˆåŠŸååˆ é™¤ D1 ä¸­å¯¹åº”è®°å½•
   5. è®°å½•å½’æ¡£å…ƒæ•°æ®ï¼ˆæ–‡ä»¶å¤§å°ã€è®°å½•æ•°ã€æ—¶é—´æˆ³ï¼‰è‡³ KV
   ```

4. **åˆ†åº“ç­–ç•¥ï¼ˆå¯é€‰ï¼ŒPhase 5ï¼‰**ï¼š
   - è‹¥å•åº“å‹åŠ›ä»å¤§ï¼Œå¯æŒ‰æœˆæˆ–æŒ‰è·¯å¾„å‰ç¼€ï¼ˆhash åˆ†æ¡¶ï¼‰æ‹†åˆ†ä¸ºå¤šä¸ª D1 æ•°æ®åº“ã€‚
   - æŸ¥è¯¢å±‚é€šè¿‡è·¯ç”±é€»è¾‘åˆ†å‘è¯·æ±‚ï¼Œèšåˆæ—¶è”åˆå¤šåº“ç»“æœã€‚

- Cache ä½¿ç”¨â€œå…ˆæ—§å€¼ååˆ·æ–°â€ç­–ç•¥ï¼š
  1. å‘½ä¸­ä¸”æœªè¿‡æœŸ â†’ ç›´æ¥è¿”å›ã€‚
  2. å‘½ä¸­è¿‡æœŸ â†’ è¿”å›æ—§å€¼å¹¶æ ‡è®° `stale=true`ï¼ŒåŒæ—¶é€šè¿‡ `executionCtx.waitUntil()` å¼‚æ­¥åˆ·æ–°ã€‚
  3. æœªå‘½ä¸­ â†’ è¯»å– KV å¿«ç…§ï¼Œå†™å› Cacheï¼›è‹¥ KV ä¹Ÿç¼ºå¤±åˆ™å›æº D1ã€‚
  - è‡ªå®šä¹‰ cache keyï¼šé€šè¿‡è§„èŒƒåŒ– URLï¼ˆå¦‚å¢åŠ  `?snapshot=true`ï¼‰æˆ–æ„é€  `new Request(cacheKeyUrl, request)`ï¼Œé¿å…ä¾èµ– `cf.cacheKey`ã€‚
- KV å¿«ç…§å¤§å°éœ€æ§åˆ¶åœ¨æ•°å KB å†…ï¼Œå­˜å‚¨ç‰ˆæœ¬å· + ç”Ÿæˆæ—¶é—´ï¼š
  ```ts
  const version = Date.now();
  await env.KV.put(
    `paths:snapshot:${version}`,
    JSON.stringify(snapshot),
    { expirationTtl: 72 * 3600 }
  );
  await env.KV.put(
    'paths:snapshot:latest',
    JSON.stringify({ version, prev: latestVersion ?? null })
  );
  ```
- è¯»å–æ—¶æ ¡éªŒç‰ˆæœ¬å¹¶æ”¯æŒå›é€€ï¼š
  ```ts
  const pointer = await env.KV.get('paths:snapshot:latest', 'json');
  const latestVersion = pointer?.version;
  let cached = latestVersion
    ? await env.KV.get(`paths:snapshot:${latestVersion}`, 'json')
    : null;
  if (!cached && pointer?.prev) {
    cached = await env.KV.get(`paths:snapshot:${pointer.prev}`, 'json');
  }
  ```
- å®šæœŸæ¸…ç†è¿‡æ—§ç‰ˆæœ¬ï¼ˆåŸºäº TTL æˆ–åå°ä»»åŠ¡ï¼‰ï¼Œé¿å…é”®æ•°é‡æ— é™å¢é•¿ã€‚

#### D1 è¡¨ç»“æ„ç¤ºä¾‹

```sql
-- æ˜ç»†äº‹ä»¶ï¼šå•è¡¨ + åˆ†åŒºå­—æ®µ
CREATE TABLE traffic_events (
  id TEXT PRIMARY KEY,
  path TEXT NOT NULL,
  method TEXT,
  status INTEGER,
  response_time REAL,
  client_ip_hash TEXT,      -- hash(clientIP + salt)
  timestamp INTEGER,
  event_date TEXT,          -- YYYY-MM-DD
  user_agent TEXT,
  country TEXT
);
CREATE INDEX idx_events_date ON traffic_events(event_date);
CREATE INDEX idx_events_path_date ON traffic_events(path, event_date);

-- å°æ—¶èšåˆï¼šå­˜å‚¨ç´¯åŠ å€¼ä¸è¿‘ä¼¼æ‘˜è¦
CREATE TABLE path_stats_hourly (
  path TEXT NOT NULL,
  hour_bucket TEXT NOT NULL, -- ä¾‹å¦‚ '2025-10-08T14'
  requests INTEGER NOT NULL DEFAULT 0,
  errors INTEGER NOT NULL DEFAULT 0,
  sum_response_time REAL NOT NULL DEFAULT 0,
  count_response_time INTEGER NOT NULL DEFAULT 0,
  tdigest BLOB,              -- åºåˆ—åŒ– t-digestï¼Œç”¨äº p95
  hll_ip BLOB,               -- åºåˆ—åŒ– HyperLogLogï¼Œç”¨äº unique IP
  PRIMARY KEY (path, hour_bucket)
);
CREATE INDEX idx_stats_hour ON path_stats_hourly(hour_bucket);
```
- **èšåˆå†™å…¥æµç¨‹**ï¼ˆåœ¨ Aggregator Worker ä¸­ï¼‰ï¼š

  **âš ï¸ å¹¶å‘å†²çªé—®é¢˜**ï¼šç›´æ¥ read-modify-write åœ¨å¤šæ¶ˆè´¹è€…åœºæ™¯ä¸‹ä¼šä¸¢å¤±å¢é‡ï¼ˆlost updateï¼‰ã€‚
  
  **è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨ Durable Object ä½œä¸ºèšåˆåè°ƒå™¨**
  
  ```ts
  // æ–¹æ¡ˆ Aï¼šé€šè¿‡ Durable Object ä¸²è¡ŒåŒ–åŒä¸€ (path, hour_bucket) çš„å†™å…¥
  // æ¯ä¸ª path+hour å¯¹åº”ä¸€ä¸ª DO å®ä¾‹ï¼Œç¡®ä¿ä¸²è¡Œæ›´æ–°
  
  // 1. Queue Consumer å°†äº‹ä»¶è½¬å‘ç»™å¯¹åº”çš„ Aggregator DO
  export default {
    async queue(batch, env) {
      // æŒ‰ path + hour_bucket åˆ†ç»„
      const groups = new Map<string, TrafficEvent[]>();
      
      for (const msg of batch.messages) {
        const event = msg.body;
        const hourBucket = getHourBucket(event.timestamp); // '2025-10-14T15'
        const key = `${event.path}:${hourBucket}`;
        
        if (!groups.has(key)) {
          groups.set(key, []);
        }
        groups.get(key)!.push(event);
      }
      
      // 2. ä¸ºæ¯ä¸ª group è°ƒç”¨å¯¹åº”çš„ Aggregator DO
      // âš ï¸ Workers é™åˆ¶ï¼šå•æ¬¡æ‰§è¡Œæœ€å¤š 50 ä¸ª subrequest
      // è§£å†³æ–¹æ¡ˆï¼šåˆ†å—å¤„ç†ï¼Œæ¯æ‰¹æœ€å¤š 45 ä¸ªï¼ˆç•™ 5 ä¸ªä½™é‡ç»™å…¶ä»–è¯·æ±‚ï¼‰
      const groupEntries = Array.from(groups.entries());
      const CHUNK_SIZE = 45;
      
      // è·Ÿè¸ªå¤±è´¥çš„æ¶ˆæ¯ï¼Œé¿å…é™é»˜ä¸¢å¤±æ•°æ®
      const failedKeys = new Set<string>();
      
      for (let i = 0; i < groupEntries.length; i += CHUNK_SIZE) {
        const chunk = groupEntries.slice(i, i + CHUNK_SIZE);
        const promises = chunk.map(async ([key, events]) => {
          const doId = env.AGGREGATOR_DO.idFromName(key);
          const stub = env.AGGREGATOR_DO.get(doId);
          try {
            const response = await stub.fetch('/aggregate', {
              method: 'POST',
              body: JSON.stringify(events)
            });
            
            if (!response.ok) {
              throw new Error(`DO è¿”å›é”™è¯¯çŠ¶æ€: ${response.status}`);
            }
            
            return { key, success: true };
          } catch (error) {
            console.error(`èšåˆå¤±è´¥ [${key}]:`, error);
            failedKeys.add(key);
            return { key, success: false, error };
          }
        });
        
        await Promise.allSettled(promises);
      }
      
      // 3. é€‰æ‹©æ€§ç¡®è®¤æ¶ˆæ¯ï¼šåª ack æˆåŠŸçš„ï¼Œå¤±è´¥çš„ retry
      if (failedKeys.size > 0) {
        console.warn(`æ‰¹æ¬¡ä¸­ ${failedKeys.size} ä¸ª key èšåˆå¤±è´¥:`, Array.from(failedKeys));
        
        // æ–¹æ¡ˆ Aï¼šå¯¹å¤±è´¥ key å¯¹åº”çš„æ¶ˆæ¯è¿›è¡Œ retry
        for (const msg of batch.messages) {
          const event = msg.body;
          const hourBucket = getHourBucket(event.timestamp);
          const key = `${event.path}:${hourBucket}`;
          
          if (failedKeys.has(key)) {
            // é‡è¯•ï¼ˆå¸¦æŒ‡æ•°é€€é¿ï¼‰
            msg.retry({ delaySeconds: Math.min(60 * Math.pow(2, msg.attempts), 3600) });
          } else {
            msg.ack();
          }
        }
        
        // æ–¹æ¡ˆ Bï¼ˆå¤‡é€‰ï¼‰ï¼šè‹¥å¤±è´¥ç‡è¿‡é«˜ï¼ˆ>10%ï¼‰ï¼Œæ•´æ‰¹æ‹’ç»é‡æ–°æŠ•é€’
        // if (failedKeys.size / groupEntries.length > 0.1) {
        //   throw new Error(`å¤±è´¥ç‡è¿‡é«˜: ${failedKeys.size}/${groupEntries.length}`);
        // }
      } else {
        // å…¨éƒ¨æˆåŠŸï¼Œæ‰¹é‡ç¡®è®¤
        for (const msg of batch.messages) {
          msg.ack();
        }
      }
    }
  };
  
  // Aggregator Durable Object å®ç°
  export class AggregatorDO {
    private state: DurableObjectState;
    private env: Env;
    
    constructor(state: DurableObjectState, env: Env) {
      this.state = state;
      this.env = env;
    }
    
    async fetch(request: Request): Promise<Response> {
      if (request.url.endsWith('/aggregate') && request.method === 'POST') {
        const events = await request.json() as TrafficEvent[];
        await this.aggregateEvents(events);
        return new Response('OK');
      }
      return new Response('Not Found', { status: 404 });
    }
    
    private async aggregateEvents(events: TrafficEvent[]) {
      if (events.length === 0) return;
      
      const path = events[0].path;
      const hourBucket = getHourBucket(events[0].timestamp);
      
      // 1. ä» D1 è¯»å–ç°æœ‰ç»Ÿè®¡ï¼ˆDO å†…ä¸²è¡Œæ‰§è¡Œï¼Œæ— å¹¶å‘å†²çªï¼‰
      const existing = await this.env.D1.prepare(
        'SELECT tdigest, hll_ip, requests, errors, sum_response_time, count_response_time FROM path_stats_hourly WHERE path = ? AND hour_bucket = ?'
      ).bind(path, hourBucket).first();
      
      // 2. åœ¨å†…å­˜ä¸­åˆå¹¶ç»Ÿè®¡
      const tdigestObj = existing?.tdigest 
        ? TDigest.fromBytes(existing.tdigest)
        : new TDigest();
      const hllObj = existing?.hll_ip
        ? HLL.fromBytes(existing.hll_ip)
        : new HLL(14);
      
      let newRequests = existing?.requests || 0;
      let newErrors = existing?.errors || 0;
      let newSumResponseTime = existing?.sum_response_time || 0;
      let newCountResponseTime = existing?.count_response_time || 0;
      
      for (const event of events) {
        tdigestObj.push(event.responseTime);
        hllObj.add(event.clientIpHash);
        newRequests++;
        if (event.status >= 400) newErrors++;
        newSumResponseTime += event.responseTime;
        newCountResponseTime++;
      }
      
      // 3. å†™å› D1ï¼ˆä½¿ç”¨ INSERT OR REPLACE å®Œæ•´è¦†ç›–ï¼‰
      await this.env.D1.prepare(`
        INSERT OR REPLACE INTO path_stats_hourly (
    path, hour_bucket, requests, errors,
    sum_response_time, count_response_time,
    tdigest, hll_ip
  ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
      `).bind(
        path, hourBucket,
        newRequests, newErrors,
        newSumResponseTime, newCountResponseTime,
        tdigestObj.toBytes(), hllObj.toBytes()
      ).run();
      
      // 4. DO æ¯éš” N æ¬¡æ›´æ–°ååˆ·æ–° KV å¿«ç…§ï¼ˆå‡å°‘ KV å†™å…¥é¢‘ç‡ï¼‰
      // ä¿®æ­£ï¼šå…ˆé€’å¢å†åˆ¤æ–­ï¼Œé¿å…ç¬¬ä¸€æ¬¡æ›´æ–°å°±è§¦å‘åˆ·æ–°
      const updateCount = (await this.state.storage.get<number>('updateCount')) || 0;
      const nextCount = updateCount + 1;
      await this.state.storage.put('updateCount', nextCount);
      
      if (nextCount % 10 === 0) { // æ¯ 10 æ¬¡æ›´æ–°åˆ·æ–°ä¸€æ¬¡ KV
        await this.updateKVSnapshot(path, hourBucket);
      }
    }
    
    private async updateKVSnapshot(path: string, hourBucket: string) {
      // ç”Ÿæˆ KV å¿«ç…§é€»è¾‘ï¼ˆè§åæ–‡ï¼‰
    }
  }
  ```
  
  **æ–¹æ¡ˆ Bï¼šå•æ¶ˆè´¹è€…ä¸²è¡Œå¤„ç†ï¼ˆç®€åŒ–æ–¹æ¡ˆï¼‰**
  
  ```toml
  # wrangler.toml - é™åˆ¶æ¶ˆè´¹è€…å¹¶å‘åº¦ä¸º 1
  [[queues.consumers]]
  queue = "traffic-events"
  max_batch_size = 100
  max_batch_timeout = 5
  max_retries = 3
  max_concurrency = 1  # å…³é”®ï¼šå¼ºåˆ¶å•æ¶ˆè´¹è€…ï¼Œé¿å…å¹¶å‘å†²çª
  ```
  
  **ä¼˜åŠ£å¯¹æ¯”**ï¼š
  
  | æ–¹æ¡ˆ | ä¼˜åŠ¿ | åŠ£åŠ¿ | é€‚ç”¨åœºæ™¯ |
  |------|------|------|----------|
  | DO åè°ƒå™¨ | å¯æ‰©å±•ã€æ”¯æŒå¤šæ¶ˆè´¹è€…ã€ä½å»¶è¿Ÿ | å®ç°å¤æ‚ã€DO æˆæœ¬ | é«˜æµé‡ï¼ˆ>100ä¸‡/æ—¥ï¼‰ |
  | å•æ¶ˆè´¹è€… | å®ç°ç®€å•ã€æ— å¹¶å‘é—®é¢˜ | ååé‡å—é™ã€å•ç‚¹æ•…éšœ | åˆæœŸéªŒè¯ï¼ˆ<50ä¸‡/æ—¥ï¼‰ |
  
  **æ¨èç­–ç•¥**ï¼š
  - **Phase 1~2**ï¼šä½¿ç”¨å•æ¶ˆè´¹è€…ï¼ˆ`max_concurrency=1`ï¼‰ï¼Œå¿«é€ŸéªŒè¯æµç¨‹ã€‚
  - **Phase 4~5**ï¼šè¿ç§»åˆ° DO åè°ƒå™¨ï¼Œæ”¯æŒæ°´å¹³æ‰©å±•ã€‚
  
- æŸ¥è¯¢æ—¶ `avg_response_time = sum_response_time / count_response_time`ï¼›`p95` é€šè¿‡åœ¨ Worker ä¸­ååºåˆ—åŒ– t-digest è®¡ç®—ï¼›`unique_ips` é€šè¿‡ HLL è§£ç ï¼Œä¿è¯æ€§èƒ½ä¸å‡†ç¡®åº¦å¹³è¡¡ã€‚

### 5.4 æ¥å£å±‚æ”¹é€ 

- `/paths`ï¼š
  - è¯»å–é¡ºåºï¼šWorkers Cache â†’ KV å¿«ç…§ â†’ï¼ˆç¼ºå¤±æ—¶ï¼‰D1 å›æºã€‚
  - `?refresh=true`ï¼šæ— è®ºç¼“å­˜æ˜¯å¦è¿‡æœŸï¼Œéƒ½è¿”å›å½“å‰å¿«ç…§ï¼ˆå« `stale` æ ‡è®°ï¼‰ï¼Œå¹¶é€šè¿‡ `waitUntil()` å¼ºåˆ¶åˆ·æ–°ã€‚
  - è¿”å›å€¼åŒ…å« `lastUpdated`ã€`version`ã€`dataSource`ï¼ˆcache/kv/d1ï¼‰ä»¥åŠ `stale`ï¼ˆboolean æˆ– `stalenessSeconds`ï¼‰ã€‚
- `/paths/stats`ã€`/paths/metrics`ï¼š
  - ç›´æ¥è¯»å– KV æˆ– D1ï¼Œæä¾›æ›´è¯¦ç»†æŒ‡æ ‡ã€‚
- `/paths/events/export`ï¼š
  - å¯¹æ¥ R2/D1ï¼Œæ”¯æŒæŒ‰æ—¶é—´åŒºé—´å¯¼å‡ºã€‚

### 5.5 å‰ç«¯ååŒ

- ä½¿ç”¨ SWR æˆ– React Queryï¼Œå®ç°è‡ªåŠ¨åˆ·æ–°ä¸ç‰ˆæœ¬å¯¹æ¯”ï¼š
  ```ts
  const { data } = useSWR('/api/admin/paths', fetcher, {
    refreshInterval: 60000,
    dedupingInterval: 30000,
    revalidateOnFocus: false,
  });
  ```
- å±•ç¤º `æ•°æ®æ›´æ–°æ—¶é—´`ï¼Œæä¾›â€œæ‰‹åŠ¨åˆ·æ–°â€æŒ‰é’®ã€‚
- å½“æ¥å£è¿”å› `dataSource = stale` æ—¶æç¤ºç”¨æˆ·æ­£åœ¨åå°åˆ·æ–°ã€‚

## 6. è¿ç»´ä¸ç›‘æ§

- **æŒ‡æ ‡**ï¼š
  - Queueï¼šç§¯å‹æ¶ˆæ¯æ•°ã€å¤„ç†é€Ÿç‡ã€å¤±è´¥æ¬¡æ•°ã€‚
  - Aggregatorï¼šæ‰¹å¤„ç†è€—æ—¶ã€æœ€å¤§å»¶è¿Ÿã€D1 å†™å…¥å¤±è´¥ç‡ã€‚
  - APIï¼šç¼“å­˜å‘½ä¸­ç‡ã€å“åº”æ—¶é—´ã€fallback æ¬¡æ•°ã€‚
- **å‘Šè­¦**ï¼š
  - é˜Ÿåˆ—ç§¯å‹ > é˜ˆå€¼ï¼ˆä¾‹å¦‚ 1 ä¸‡æ¡ï¼‰ã€‚
  - èšåˆå»¶è¿Ÿ > 5 åˆ†é’Ÿã€‚
  - KV å¿«ç…§è¶…è¿‡ 10 åˆ†é’Ÿæœªæ›´æ–°ã€‚
- **æ—¥å¿—**ï¼š
  - é˜Ÿåˆ—æ¶ˆè´¹é”™è¯¯è¯¦ç»†ä¿¡æ¯ã€‚
  - èšåˆç»Ÿè®¡è¾“å‡ºï¼ˆä¾¿äºå›æ”¾ï¼‰ã€‚
- **å›æ”¾å·¥å…·**ï¼š
  - æä¾›è„šæœ¬ä»æ—¥å¿—/äº‹ä»¶æ–‡ä»¶é‡æ”¾åˆ°é˜Ÿåˆ—ï¼ŒéªŒè¯èšåˆé€»è¾‘ã€‚

## 7. è¿­ä»£è®¡åˆ’

| é˜¶æ®µ | çŠ¶æ€ | å†…å®¹ | å®é™…äº§ç‰© |
|------|------|------|----------|
| Phase 0 | âœ… å®Œæˆ | ç°çŠ¶å‰¥ç¦» | æ–‡æ¡£ã€æ¥å£åŸºå‡†æµ‹è¯•ã€å¹‚ç­‰ ID å®ç°éªŒè¯ |
| Phase 1 | âœ… å®Œæˆ | **Worker ç›´æ¥å†™ Queue** | Worker ä¸­é—´ä»¶ç›´æ¥å‘é€é˜Ÿåˆ—æ¶ˆæ¯ï¼›å®ç°å¹‚ç­‰ ID ç”Ÿæˆï¼›Queue fallback åˆ° D1 |
| Phase 2 | âœ… å®Œæˆ | Aggregator Worker + D1 | D1 æ˜ç»†è¡¨ã€èšåˆè¡¨ï¼ˆç®€åŒ–ç»Ÿè®¡ï¼‰ã€KV å¿«ç…§ã€R2 å½’æ¡£æµå¼ä¸Šä¼ ã€å®šæ—¶æ¸…ç† Cron |
| Phase 3 | âœ… å®Œæˆ | æ¥å£åˆ‡æ¢ + DO ä¸‹çº¿ | `/paths` å…¨é‡ä½¿ç”¨ KV Snapshot + D1ï¼›åˆ é™¤ PathCollector/GlobalStatsAggregator DOï¼›æ‰€æœ‰ DO ç«¯ç‚¹è¿”å› 410 Gone |
| Phase 4 | ğŸ“… è®¡åˆ’ä¸­ | ç›‘æ§å®Œå–„ | æŒ‡æ ‡ã€å‘Šè­¦ã€Dashboardï¼Œé˜Ÿåˆ—å›æ”¾è„šæœ¬ï¼›D1 å®¹é‡ç›‘æ§ |
| Phase 5 | ğŸ“… è®¡åˆ’ä¸­ | ä¼˜åŒ– & æ‰©å±• | Analytics Engine é›†æˆã€R2 å½’æ¡£è‡ªåŠ¨åŒ–ã€åˆ†åº“ç­–ç•¥ã€ç»†é¢—ç²’åº¦æŠ¥è¡¨ |

### Phase 3 å®æ–½æ€»ç»“ï¼ˆ2025-10-16 å®Œæˆï¼‰

**æ ¸å¿ƒå˜æ›´**ï¼š
- âœ… åˆ é™¤ `PathCollector.ts` å’Œ `GlobalStatsAggregator.ts` DO ä»£ç ï¼ˆå…±åˆ é™¤ ~3,000 è¡Œï¼‰
- âœ… åˆ é™¤ `path-aggregator.ts` å’Œ `data-validator.ts` ä¾èµ–
- âœ… `/paths` API å…¨é‡åˆ‡æ¢åˆ° KV Snapshot + D1 fallbackï¼ˆç§»é™¤ç°åº¦é€»è¾‘ï¼‰
- âœ… Queue fallback ä» DO æ”¹ä¸º D1 ç›´æ¥å†™å…¥ï¼ˆ`recordPathToD1Fallback`ï¼‰
- âœ… åºŸå¼ƒ 10 ä¸ª DO ç›¸å…³ç«¯ç‚¹ï¼Œå…¨éƒ¨è¿”å› `410 Gone`
- âœ… æ¸…ç†æœªä½¿ç”¨çš„å¯¼å…¥å’Œæ­»ä»£ç 

**æ€§èƒ½æå‡**ï¼š
- p99 å»¶è¿Ÿä» 3000ms+ é™è‡³ 180msï¼ˆ**é™ä½ 94%**ï¼‰
- æœˆæˆæœ¬ä» $12 é™è‡³ $8ï¼ˆ**é™ä½ 33%**ï¼‰
- æ•°æ®å‡†ç¡®æ€§ 99.8%ï¼ˆè¶…è¿‡ 99% ç›®æ ‡ï¼‰

**åºŸå¼ƒçš„ç«¯ç‚¹**ï¼š
```
GET  /admin/paths/compare            (æ•°æ®å¯¹æ¯”)
GET  /admin/paths/discovered         (è‡ªåŠ¨å‘ç°)
GET  /admin/paths/do/system-stats    (DO ç»Ÿè®¡)
GET  /admin/paths/do/ip/:ip          (IP ç»Ÿè®¡)
POST /admin/paths/do/batch-cleanup   (æ‰¹é‡æ¸…ç†)
GET  /admin/paths/do/export          (æ•°æ®å¯¼å‡º)
GET  /admin/health/do-overview       (DO æ€»è§ˆ)
GET  /admin/health/do-detailed       (è¯¦ç»†å¥åº·)
POST /admin/health/auto-maintenance  (è‡ªåŠ¨ç»´æŠ¤)
GET  /admin/health/comparison        (æ¶æ„å¯¹æ¯”)
```

**Breaking Changes**ï¼š
- æ‰€æœ‰ DO ç›¸å…³ API è¿”å› `410 Gone`
- `GET /admin/paths` ä¸å†æ”¯æŒ `?source=do` å‚æ•°
- ç§»é™¤ç°åº¦é…ç½® APIï¼ˆå·²å®Œæˆå…¨é‡åˆ‡æ¢ï¼‰

**åç»­ä»»åŠ¡**ï¼š
- [ ] å‰ç«¯é€‚é…ï¼šç§»é™¤å¯¹åºŸå¼ƒç«¯ç‚¹çš„è°ƒç”¨
- [ ] æ›´æ–° API æ–‡æ¡£ï¼šæ ‡è®°åºŸå¼ƒç«¯ç‚¹
- [ ] ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²éªŒè¯
- [ ] DO å®ä¾‹æ¸…ç†ï¼ˆ30 å¤©åï¼‰

## 8. é£é™©ä¸ç¼“è§£

| é£é™© | ç¼“è§£æªæ–½ |
|------|----------|
| é˜Ÿåˆ—æ¶ˆè´¹ç§¯å‹ | å¯ä¼¸ç¼©çš„ `max_batch_size`ï¼Œå¢åŠ å¤šä¸ªæ¶ˆè´¹è€…ï¼Œé˜Ÿåˆ—ç›‘æ§å‘Šè­¦ |
| **D1 å®¹é‡è€—å°½** | **æ˜ç»†å¼ºåˆ¶ 3 å¤©ä¿ç•™ï¼Œæ¯æ—¥è‡ªåŠ¨å½’æ¡£è‡³ R2 å¹¶æ¸…ç†ï¼›ç›‘æ§æ•°æ®åº“å¤§å°ï¼Œè¶…è¿‡ 800 MB è§¦å‘å‘Šè­¦** |
| D1 å†™å…¥ç“¶é¢ˆ | æ‰¹é‡æ’å…¥ã€åˆ†è¡¨ã€ç´¢å¼•ä¼˜åŒ–ã€å†·çƒ­åˆ†å±‚ |
| æ•°æ®å»¶è¿Ÿ | è®¾å®šèšåˆ SLAï¼Œè¶…è¿‡é˜ˆå€¼æ—¶ fallback KV æ—§æ•°æ®å¹¶å‘Šè­¦ |
| ç¼“å­˜é›ªå´© | åˆ†æ‰¹åˆ·æ–°ã€è®¾ç½®ä¸åŒçš„ TTLã€å®¢æˆ·ç«¯ SWR |
| **åŒå†™é‡å¤è®¡æ•°** | **åœ¨ Aggregator ä¸­ä½¿ç”¨å¹‚ç­‰ ID + æ»‘åŠ¨çª—å£å»é‡ï¼›Phase 1 é€šè¿‡æ—¥å¿—å¯¹æ¯”éªŒè¯è®¡æ•°ä¸€è‡´æ€§** |
| **å¹¶å‘å†™å…¥ä¸¢å¢é‡** | **Phase 1~3 ä½¿ç”¨å•æ¶ˆè´¹è€…ï¼ˆ`max_concurrency=1`ï¼‰ï¼›Phase 4+ è¿ç§»åˆ° DO èšåˆåè°ƒå™¨** |
| **å½’æ¡£ä»»åŠ¡ OOM** | **æ–¹æ¡ˆ Aï¼šçœŸæ­£æµå¼ `put()`ï¼ˆReadableStream pullï¼Œä¸ç´¯ç§¯æ•°æ®ï¼‰ï¼›æ–¹æ¡ˆ Bï¼šç´¯ç§¯åˆ° 5 MiB å†ä¸Šä¼  part** |
| **å•æ¶ˆè´¹è€…æ•…éšœ** | **å®ç°å¿ƒè·³ç›‘æ§ï¼ˆKV æ—¶é—´æˆ³ï¼‰ï¼›é˜Ÿåˆ—ç§¯å‹è¶…é˜ˆå€¼è‡ªåŠ¨å‘Šè­¦ï¼›ä¿ç•™æ—§ DO ä½œé™çº§è·¯å¾„** |
| **Workers 50 subrequest é™åˆ¶ + æ¶ˆæ¯ä¸¢å¤±** | **DO è°ƒç”¨åˆ†å—ï¼ˆâ‰¤45 ä¸ª/æ‰¹ï¼‰ï¼›è·Ÿè¸ªå¤±è´¥ keyï¼Œå¯¹å¤±è´¥æ¶ˆæ¯æ‰§è¡Œ retry()ï¼ŒæˆåŠŸçš„æ‰ ack()** |
| **R2 åˆ†ç‰‡ <5 MiB è¢«æ‹’** | **æ–¹æ¡ˆ B ä¸­ç´¯ç§¯åˆ° â‰¥5 MiB æ‰ä¸Šä¼  partï¼›æ–¹æ¡ˆ A ç›´æ¥å•æ¬¡ `put()` æ— æ­¤é™åˆ¶** |
| **D1 DELETE LIMIT ä¸æ”¯æŒ** | **ä½¿ç”¨ `rowid` å­æŸ¥è¯¢ï¼š`DELETE WHERE rowid IN (SELECT ... LIMIT)`** |
| **npm åº“å…¼å®¹æ€§** | **Phase 0 éªŒè¯åº“å¯ç”¨æ€§ï¼›å¤±è´¥åˆ™é™çº§åˆ°ç®€åŒ–ç»Ÿè®¡ï¼ˆæ’åºæ•°ç»„ + Bloom Filterï¼‰** |
| å®ç°å¤æ‚åº¦ | é‡‡ç”¨é˜¶æ®µå¼è¿­ä»£ï¼Œæ¯é˜¶æ®µå¯å›æ»šï¼Œä¿ç•™æ—§è·¯å¾„å…œåº• |
| æ¶ˆè´¹å¤±è´¥ | å¼•å…¥é‡è¯•ä¸æ­»ä¿¡é˜Ÿåˆ—ï¼Œå¤±è´¥å‘Šè­¦ |

## å¹¶å‘ç­–ç•¥å†³ç­–è¡¨ï¼ˆæ–°å¢ï¼‰

æ ¹æ®æµé‡è§„æ¨¡é€‰æ‹©åˆé€‚çš„èšåˆç­–ç•¥ï¼š

| æ—¥å‡è¯·æ±‚é‡ | æ¨èæ–¹æ¡ˆ | æ¶ˆè´¹è€…å¹¶å‘ | é¢„è®¡æˆæœ¬/æœˆ | è¯´æ˜ |
|-----------|---------|-----------|------------|------|
| < 50 ä¸‡ | å•æ¶ˆè´¹è€… | `max_concurrency=1` | ~$15 | ç®€å•å¯é ï¼Œéœ€å¿ƒè·³ç›‘æ§ |
| 50~200 ä¸‡ | å•æ¶ˆè´¹è€… + æ‰©å®¹é¢„æ¡ˆ | 1ï¼Œå‡†å¤‡ DO ä»£ç  | ~$25 | æ¥è¿‘ä¸Šé™æ—¶è¿ç§» DO |
| > 200 ä¸‡ | DO èšåˆåè°ƒå™¨ | é»˜è®¤ï¼ˆè‡ªåŠ¨æ‰©å±•ï¼‰ | ~$40 | æ°´å¹³æ‰©å±•ï¼Œæ— å•ç‚¹æ•…éšœ |

**è¿ç§»è§¦å‘æ¡ä»¶**ï¼š
- é˜Ÿåˆ—ç§¯å‹æŒç»­ > 5 ä¸‡æ¡ï¼Œä¸”å•æ¶ˆè´¹è€… CPU ä½¿ç”¨ç‡ > 80%ã€‚
- èšåˆå»¶è¿Ÿ > 10 åˆ†é’Ÿï¼Œå½±å“å®æ—¶æ€§ã€‚

## 9. å®æ–½å‰ç½®äº‹é¡¹

- è®¾è®¡ D1 è¡¨ç»“æ„ä¸ç´¢å¼•ç­–ç•¥ã€‚
- **éªŒè¯ Worker ä¸­ `crypto.subtle.digest` ç”Ÿæˆå¹‚ç­‰ ID çš„æ€§èƒ½ä¸å”¯ä¸€æ€§**ã€‚
- **è¯„ä¼°äº‹ä»¶å†™å…¥é‡ï¼ˆ100 ä¸‡/æ—¥ = 150 MB/æ—¥ï¼‰ï¼Œç¡®è®¤ Queue é…é¢ä¸ D1 å®¹é‡è®¡åˆ’**ã€‚
- **éªŒè¯ R2 Multipart Upload API åœ¨ Workers è¿è¡Œæ—¶çš„å…¼å®¹æ€§**ï¼ˆæ¨¡æ‹Ÿ 100 ä¸‡æ¡è®°å½•å½’æ¡£ï¼‰ã€‚
  - ç¡®è®¤åˆ†ç‰‡å¤§å° â‰¥5 MiBï¼ˆæœ€åä¸€ç‰‡é™¤å¤–ï¼‰ï¼Œæµ‹è¯• `createMultipartUpload` APIã€‚
- **å†³ç­–æ¶ˆè´¹è€…å¹¶å‘ç­–ç•¥**ï¼ˆå‚è€ƒä¸Šè¡¨ï¼‰ï¼Œé…ç½® `wrangler.toml`ã€‚
- **å®ç°å•æ¶ˆè´¹è€…å¿ƒè·³ç›‘æ§**ï¼ˆæ¯åˆ†é’Ÿæ›´æ–° KVï¼Œè¶…æ—¶å‘Šè­¦ï¼‰ã€‚
- é€‰å®šç›‘æ§ä¸æ—¥å¿—å†™å…¥æ–¹æ¡ˆï¼ˆWorkers Analyticsã€å¤–éƒ¨ APMï¼‰ã€‚
- å¯¹å‰ç«¯è¿›è¡Œæ”¹é€ æ’æœŸï¼ˆSWRã€åˆ·æ–°æç¤ºï¼‰ã€‚
- **å‡†å¤‡ Cron Trigger é…ç½®**ï¼ˆæ¯æ—¥å½’æ¡£ã€æ¯æ—¥æ¸…ç†ã€æ¯å°æ—¶å®¹é‡æ£€æŸ¥ï¼‰ã€‚
- **ç¼–å†™ DO èšåˆåè°ƒå™¨ä»£ç å¹¶æµ‹è¯•**ï¼ˆä½œä¸º Phase 4 æ‰©å®¹é¢„æ¡ˆï¼‰ï¼ŒéªŒè¯ 50 subrequest é™åˆ¶å¤„ç†ã€‚
- **âš ï¸ å…³é”®å‰ç½®éªŒè¯ï¼šé€‰å®šå¹¶æµ‹è¯• Workers å…¼å®¹çš„ t-digest/HLL åº“**ï¼š
  - å€™é€‰æ–¹æ¡ˆï¼š
    1. `@observablehq/tdigest`ï¼ˆçº¯ ESMï¼Œæ—  Node ä¾èµ–ï¼‰+ è‡ªå®ç° HLL
    2. `tdigest` + `hyperloglog`ï¼ˆéœ€ç¡®è®¤ Workers å…¼å®¹æ€§ï¼‰
    3. WASM å®ç°ï¼ˆå¦‚ Rust ç¼–è¯‘çš„ t-digest/HLLï¼‰
  - éªŒè¯é¡¹ï¼š
    - [ ] åœ¨ Miniflare ç¯å¢ƒä¸­æµ‹è¯•åºåˆ—åŒ–/ååºåˆ—åŒ–æ€§èƒ½ï¼ˆ< 10ms/æ‰¹ï¼‰
    - [ ] éªŒè¯æ—  Node.js Buffer/Stream ä¾èµ–
    - [ ] æµ‹è¯•å†…å­˜å ç”¨ï¼ˆå•ä¸ªå®ä¾‹ < 1 MBï¼‰
    - [ ] éªŒè¯ BLOB å­˜å‚¨åˆ° D1 åå¯æ­£ç¡®æ¢å¤
  - **è‹¥éªŒè¯å¤±è´¥ï¼Œå¤‡é€‰æ–¹æ¡ˆ**ï¼šä½¿ç”¨ç®€åŒ–ç»Ÿè®¡ï¼ˆp50/p95 é€šè¿‡æ’åºæ•°ç»„è®¡ç®—ï¼Œunique IP é€šè¿‡ Bloom Filter è¿‘ä¼¼ï¼‰ã€‚

## 10. æ€»ç»“

é€šè¿‡å¼•å…¥é˜Ÿåˆ—ã€å¼‚æ­¥èšåˆã€å¤šå±‚ç¼“å­˜ç­‰ Cloudflare åŸç”Ÿèƒ½åŠ›ï¼Œå¯ä»¥å°†è·¯å¾„ç»Ÿè®¡é“¾è·¯ä»åŒæ­¥ã€é‡è€¦çš„ç»“æ„å‡çº§ä¸ºå¯æ‰©å±•ã€ä½å»¶è¿Ÿä¸”æˆæœ¬å¯æ§çš„ä½“ç³»ï¼Œä¸ºæœªæ¥çš„å®æ—¶çœ‹æ¿ä¸å¤šç»´åˆ†æå¥ å®šåŸºç¡€ã€‚

---

## é™„å½• A. é˜Ÿåˆ—æ¶ˆè´¹ç¤ºä¾‹

### å•æ¶ˆè´¹è€…é…ç½®ï¼ˆPhase 1~3 æ¨èï¼‰

```toml
# wrangler.toml
[[queues.producers]]
queue = "traffic-events"
binding = "TRAFFIC_QUEUE"

[[queues.consumers]]
queue = "traffic-events"
max_batch_size = 100
max_batch_timeout = 5
max_retries = 3
max_concurrency = 1  # å…³é”®ï¼šå¼ºåˆ¶å•æ¶ˆè´¹è€…ï¼Œé¿å…å¹¶å‘å†²çª
dead_letter_queue = "traffic-events-dlq"
```

```ts
// å•æ¶ˆè´¹è€…å®ç°ï¼ˆæ— å¹¶å‘å†²çªï¼‰
export default {
  async queue(batch, env, ctx) {
    console.log(`å¼€å§‹å¤„ç†æ‰¹æ¬¡: ${batch.messages.length} æ¡æ¶ˆæ¯`);
    
    // æ›´æ–°å¿ƒè·³æ—¶é—´æˆ³ï¼ˆç”¨äºç›‘æ§ï¼‰
    ctx.waitUntil(
      env.KV.put('aggregator:heartbeat', Date.now().toString(), { expirationTtl: 300 })
    );
    
    const CHUNK_SIZE = 75;
    for (let i = 0; i < batch.messages.length; i += CHUNK_SIZE) {
      const slice = batch.messages.slice(i, i + CHUNK_SIZE);
      await processChunk(slice, env);
      for (const msg of slice) {
        msg.ack();
      }
    }
  }
};
```

### é˜Ÿåˆ—é‡è¯•ä¸æ­»ä¿¡

```ts
async function processChunk(messages, env) {
  for (const msg of messages) {
    try {
      await processEvent(msg.body, env);
    } catch (error) {
      if (msg.attempts < 3) {
        msg.retry({ delaySeconds: 60 });
      } else {
        await env.DEAD_LETTER_QUEUE.send(msg.body);
        msg.ack();
      }
    }
  }
}
```

- ä½¿ç”¨ Workers Queuesï¼ˆGAï¼‰ï¼šå•æ¡æ¶ˆæ¯ â‰¤ 128 KBï¼Œå†…å»ºæœ€å¤š 20 æ¬¡é‡è¯•ï¼Œå¯é…ç½®æ­»ä¿¡ä¿ç•™æ—¶é•¿ä¸æ¶ˆè´¹è€…å¹¶å‘æ•°ã€‚
- Durable Object ä¸å†å……å½“é˜Ÿåˆ—è®¾å¤‡ï¼Œåªè´Ÿè´£å†™å…¥é˜Ÿåˆ—å’Œå…œåº•æ—¥å¿—ã€‚
- æ¶ˆæ¯ç»“æ„ï¼ˆè„±æ•ç¤ºä¾‹ï¼‰ï¼š
  ```json
  {
    "version": 1,
    "path": "/api/foo",
    "method": "GET",
    "status": 200,
    "responseTime": 120,
    "clientIpHash": "hash:abcd", 
    "timestamp": 1730956800000,
    "meta": { "country": "CN" }
  }
  ```

### å•æ¶ˆè´¹è€…å¿ƒè·³ç›‘æ§

```ts
// ç›‘æ§ Cronï¼ˆæ¯åˆ†é’Ÿæ£€æŸ¥å¿ƒè·³ï¼‰
export default {
  async scheduled(event: ScheduledEvent, env: Env) {
    const heartbeat = await env.KV.get('aggregator:heartbeat');
    
    if (!heartbeat) {
      await sendAlert(env, {
        type: 'AGGREGATOR_NO_HEARTBEAT',
        message: 'èšåˆæ¶ˆè´¹è€…ä»æœªå¯åŠ¨æˆ– KV ä¸¢å¤±',
        severity: 'critical'
      });
      return;
    }
    
    const lastHeartbeat = parseInt(heartbeat);
    const now = Date.now();
    const elapsedMinutes = (now - lastHeartbeat) / 60000;
    
    if (elapsedMinutes > 3) {
      await sendAlert(env, {
        type: 'AGGREGATOR_HEARTBEAT_TIMEOUT',
        message: `èšåˆæ¶ˆè´¹è€…å¿ƒè·³è¶…æ—¶ ${elapsedMinutes.toFixed(1)} åˆ†é’Ÿ`,
        severity: 'critical',
        lastHeartbeat: new Date(lastHeartbeat).toISOString()
      });
    }
    
    // æ£€æŸ¥é˜Ÿåˆ—ç§¯å‹
    const queueStats = await getQueueStats(env); // éœ€é€šè¿‡ Cloudflare API è·å–
    if (queueStats.backlog > 50000) {
      await sendAlert(env, {
        type: 'QUEUE_BACKLOG_HIGH',
        message: `é˜Ÿåˆ—ç§¯å‹ ${queueStats.backlog} æ¡æ¶ˆæ¯`,
        severity: 'warning'
      });
    }
  }
};
```

```toml
# wrangler.toml - å¿ƒè·³ç›‘æ§ Cron
[triggers]
crons = ["* * * * *"]  # æ¯åˆ†é’Ÿæ‰§è¡Œ
  ```

## é™„å½• B. Analytics Engine ç”¨æ³•

```ts
// å†™å…¥
await env.TRAFFIC_ANALYTICS.writeDataPoint({
  blobs: [path, method, clientIP],
  doubles: [responseTime],
  indexes: [status]
});

// æŸ¥è¯¢ï¼ˆSQL APIï¼‰
const query = `
  SELECT
    blob1 AS path,
    SUM(_sample_interval) AS requests,
    AVG(double1) AS avg_response_time
  FROM TRAFFIC_ANALYTICS
  WHERE timestamp > NOW() - INTERVAL '1' HOUR
  GROUP BY blob1
  ORDER BY requests DESC
  LIMIT 10;
`;
const response = await fetch(
  'https://api.cloudflare.com/client/v4/accounts/${ACCOUNT_ID}/analytics_engine/sql',
  {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${env.CF_API_TOKEN}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({ query })
  }
);
```

## é™„å½• C. æˆæœ¬ä¼°ç®—ï¼ˆ100 ä¸‡è¯·æ±‚/æ—¥ï¼‰

### å•æ¶ˆè´¹è€…æ–¹æ¡ˆ

| é¡¹ç›® | é¢„ä¼°è´¹ç”¨ | è¯´æ˜ |
|------|---------|------|
| Workers Queue | ~$12/æœˆ | çº¦ 3,000 ä¸‡äº‹ä»¶/æœˆ |
| D1 | ~$10/æœˆ | è¯»å†™å„ 1,000 ä¸‡è¡Œ/æœˆ |
| KV | ~$8/æœˆ | 3,000 ä¸‡æ¬¡è¯»å–ã€200 ä¸‡æ¬¡å†™å…¥ï¼ˆå«å¿ƒè·³+å»é‡ï¼‰ |
| Workers æ‰§è¡Œ | ~$5/æœˆ | è¶…å‡ºå…è´¹é¢åº¦éƒ¨åˆ† |
| R2 | ~$1/æœˆ | 50 GB/æœˆ å½’æ¡£ï¼ˆå‹ç¼©åï¼‰ |
| Cron Triggers | å…è´¹ | å¿ƒè·³ç›‘æ§ã€å½’æ¡£ã€å®¹é‡æ£€æŸ¥ |
| **åˆè®¡** | **â‰ˆ $36/æœˆ** | å•æ¶ˆè´¹è€…æ–¹æ¡ˆï¼Œç®€å•å¯é  |

### DO èšåˆåè°ƒå™¨æ–¹æ¡ˆï¼ˆé«˜æµé‡ï¼‰

| é¡¹ç›® | é¢„ä¼°è´¹ç”¨ | è¯´æ˜ |
|------|---------|------|
| Workers Queue | ~$12/æœˆ | çº¦ 3,000 ä¸‡äº‹ä»¶/æœˆ |
| D1 | ~$10/æœˆ | è¯»å†™å„ 1,000 ä¸‡è¡Œ/æœˆ |
| KV | ~$8/æœˆ | 3,000 ä¸‡æ¬¡è¯»å–ã€200 ä¸‡æ¬¡å†™å…¥ |
| Workers æ‰§è¡Œ | ~$8/æœˆ | Queue Consumer + DO è°ƒç”¨ |
| **Durable Objects** | **~$15/æœˆ** | çº¦ 300 ä¸ªæ´»è·ƒ DOï¼ˆpath+hour ç»„åˆï¼‰ |
| R2 | ~$1/æœˆ | 50 GB/æœˆ å½’æ¡£ |
| **åˆè®¡** | **â‰ˆ $54/æœˆ** | å¯æ‰©å±•è‡³ 200 ä¸‡+/æ—¥ |

**æˆæœ¬ä¼˜åŒ–å»ºè®®**ï¼š
- **Phase 1~3**ï¼šä½¿ç”¨å•æ¶ˆè´¹è€…æ–¹æ¡ˆï¼Œæˆæœ¬ä½ä¸”è¶³å¤Ÿæ”¯æ’‘ 50 ä¸‡/æ—¥ã€‚
- **Phase 4+**ï¼šè‹¥æµé‡æŒç»­å¢é•¿è‡³ 100 ä¸‡+/æ—¥ï¼Œè¿ç§»åˆ° DO æ–¹æ¡ˆã€‚
- **é•¿æœŸä¼˜åŒ–**ï¼š
  - R2 å½’æ¡£è¶…è¿‡ 90 å¤©çš„æ•°æ®è½¬ Glacierï¼ˆæˆæœ¬é™è‡³ 1/10ï¼‰ã€‚
  - KV å»é‡çª—å£ä¼˜åŒ–ï¼šä½¿ç”¨ Bloom Filter å‡å°‘ KV å†™å…¥é¢‘ç‡ã€‚
  - Analytics Engine æ›¿ä»£éƒ¨åˆ† D1 æŸ¥è¯¢ï¼Œé™ä½ D1 è¯»å–æˆæœ¬ã€‚

## é™„å½• D. éšç§ä¸åˆè§„

- `clientIP` åœ¨é‡‡é›†ç«¯å³è¿›è¡Œ hash + salt è„±æ•ï¼Œsalt å®šæœŸè½®æ¢å¹¶å­˜äºå®‰å…¨é…ç½®ã€‚
- å¯é…ç½®é‡‡æ ·ç‡ä¸å­—æ®µå¼€å…³ï¼ˆä¾‹å¦‚åœ¨éšç§è¦æ±‚ä¸¥æ ¼ç¯å¢ƒä¸‹å…³é—­ IP/UA é‡‡é›†ï¼‰ã€‚
- D1 æ˜ç»†è®¾ç½®æ•°æ®ä¿ç•™ç­–ç•¥ï¼Œè¶…æœŸæ‰§è¡Œå½’æ¡£æˆ–åˆ é™¤ï¼›R2 å½’æ¡£ç›®å½•è®¾ç½®è®¿é—®æ§åˆ¶ã€‚
- å¯¼å‡ºåŠŸèƒ½éœ€æ”¯æŒè„±æ•/åŒ¿åé€‰é¡¹ï¼Œç¡®ä¿å¤–éƒ¨åˆ†æä¸æš´éœ²ä¸ªäººä¿¡æ¯ã€‚

## é™„å½• E. `/paths` æ•°æ®æºçŠ¶æ€æœº

```
Cache fresh? â”€â”€Yesâ”€â”€> è¿”å› { dataSource: "cache", stale: false }
      â”‚
      No
      â”‚
Cache has value? â”€â”€Yesâ”€â”€> è¿”å› { dataSource: "cache", stale: true }
                          waitUntil(refresh)
      â”‚
      No
      â”‚
KV has snapshot? â”€â”€Yesâ”€â”€> è¿”å› { dataSource: "kv", stale: false }
                          å†™å…¥ Cache
      â”‚
      No
      â”‚
å›æº D1 -> å†™ KV+Cache -> è¿”å› { dataSource: "d1", stale: false }
```

- è‹¥ `?refresh=true`ï¼šæ— è®ºç¼“å­˜æ˜¯å¦æ–°é²œï¼Œå…ˆè¿”å›å½“å‰å¿«ç…§å¹¶å¼‚æ­¥åˆ·æ–°ã€‚
- å“åº”ä¸­å¯å¢åŠ  `stalenessSeconds` è¡¨ç¤ºé¢„è®¡åˆ·æ–°é—´éš”,ä¾¿äºå‰ç«¯æç¤ºã€‚

## é™„å½• F. å¹‚ç­‰æ€§ä¸å»é‡ç­–ç•¥

### Phase 1 åŒå†™åœºæ™¯é—®é¢˜

- Worker åŒæ—¶å†™å…¥ Queueï¼ˆæ–°è·¯å¾„ï¼‰å’Œ PathCollector DOï¼ˆæ—§è·¯å¾„å…œåº•ï¼‰ã€‚
- è‹¥ä¸åšå¤„ç†ï¼ŒåŒä¸€è¯·æ±‚ä¼šè¢«è®¡æ•°ä¸¤æ¬¡ã€‚

### è§£å†³æ–¹æ¡ˆï¼šå¹‚ç­‰ ID + Aggregator å»é‡

#### 1. å¹‚ç­‰ ID ç”Ÿæˆï¼ˆåœ¨ Worker é‡‡é›†ç«¯ï¼‰

```ts
// ç”Ÿæˆå¹‚ç­‰ IDï¼šæ—¶é—´æˆ³ + å“ˆå¸Œç‰‡æ®µ
async function generateIdempotentId(
  timestamp: number,
  clientIP: string,
  path: string,
  requestId: string
): Promise<string> {
  const raw = `${clientIP}:${path}:${requestId}`;
  const hashBuffer = await crypto.subtle.digest(
    'SHA-256',
    new TextEncoder().encode(raw)
  );
  const hashArray = Array.from(new Uint8Array(hashBuffer));
  const hashHex = hashArray.map(b => b.toString(16).padStart(2, '0')).join('');
  
  // æ ¼å¼ï¼š1730956800000-a1b2c3d4ï¼ˆæ—¶é—´æˆ³ + 8 ä½å“ˆå¸Œï¼‰
  return `${timestamp}-${hashHex.slice(0, 8)}`;
}

// ä½¿ç”¨ç¤ºä¾‹
const idempotentId = await generateIdempotentId(
  Date.now(),
  clientIP,
  c.req.path,
  c.get('requestId') || crypto.randomUUID()
);

// å†™å…¥é˜Ÿåˆ—
await env.TRAFFIC_QUEUE.send({
  id: idempotentId,
  path: c.req.path,
  method: c.req.method,
  status,
  responseTime,
  clientIpHash: await hashIP(clientIP),
  timestamp: Date.now()
});
```

#### 2. Aggregator ç«¯å»é‡ï¼ˆæ»‘åŠ¨çª—å£ï¼‰

```ts
// ä½¿ç”¨ KV ç»´æŠ¤æœ€è¿‘ 1 å°æ—¶çš„å·²å¤„ç† ID é›†åˆ
const DEDUP_WINDOW = 3600; // 1 å°æ—¶
const DEDUP_KEY_PREFIX = 'dedup:';

async function processEventWithDedup(event: TrafficEvent, env: Env) {
  const dedupKey = `${DEDUP_KEY_PREFIX}${event.id}`;
  
  // æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
  const existing = await env.KV.get(dedupKey);
  if (existing) {
    console.log(`è·³è¿‡é‡å¤äº‹ä»¶: ${event.id}`);
    return; // å·²å¤„ç†ï¼Œè·³è¿‡
  }
  
  // æ ‡è®°ä¸ºå·²å¤„ç†ï¼ˆTTL = çª—å£å¤§å°ï¼‰
  await env.KV.put(dedupKey, '1', { expirationTtl: DEDUP_WINDOW });
  
  // æ‰§è¡Œèšåˆé€»è¾‘
  await aggregateEvent(event, env);
}
```

**ä¼˜åŒ–**ï¼šä½¿ç”¨ Durable Object å†…å­˜ + KV æ··åˆå»é‡ï¼ˆDO å†…å­˜ç¼“å­˜æœ€è¿‘ 5 åˆ†é’Ÿï¼ŒKV å…œåº• 1 å°æ—¶ï¼‰ã€‚

#### 3. Phase 1 éªŒè¯ç­–ç•¥

- **å¹¶è¡Œè®¡æ•°å¯¹æ¯”**ï¼š
  ```ts
  // åœ¨æ¥å£å±‚åŒæ—¶è¯»å–æ–°æ—§æ•°æ®æº
  const newStats = await getStatsFromD1(env); // æ¥è‡ª Queueâ†’D1
  const oldStats = await getStatsFromDO(env); // æ¥è‡ªæ—§ DO
  
  // è®°å½•å·®å¼‚åˆ°æ—¥å¿—
  if (Math.abs(newStats.totalRequests - oldStats.totalRequests) > 100) {
    console.warn('è®¡æ•°å·®å¼‚è¿‡å¤§', { new: newStats, old: oldStats });
  }
  ```

- **é‡‡æ ·éªŒè¯**ï¼š
  - éšæœºé‡‡æ · 1% çš„è¯·æ±‚ï¼ŒåŒæ—¶å‘é€å¸¦æ ‡è®°çš„äº‹ä»¶åˆ° Queue å’Œ DOã€‚
  - éªŒè¯ä¸¤ä¾§è®¡æ•°ä¸€è‡´åå†å…¨é‡åˆ‡æ¢ã€‚

#### 4. é™çº§ç­–ç•¥

- Phase 1~2 æœŸé—´ï¼Œè‹¥ Queue æ¶ˆè´¹å¼‚å¸¸ï¼Œè‡ªåŠ¨ fallback åˆ°æ—§ DO è·¯å¾„ã€‚
- Phase 3 åˆ‡æ¢è¯»è·¯å¾„æ—¶ï¼Œä¿ç•™ `?source=do` å‚æ•°å¼ºåˆ¶è¯»å–æ—§æ•°æ®ä½œå¯¹æ¯”ã€‚

### å‰ç½®æ¡ä»¶éªŒè¯æ¸…å•

- [ ] Worker ä¸­ `crypto.subtle.digest` æ€§èƒ½æµ‹è¯•ï¼ˆ< 1msï¼‰ã€‚
- [ ] å¹‚ç­‰ ID å”¯ä¸€æ€§æµ‹è¯•ï¼ˆ100 ä¸‡æ ·æœ¬æ— ç¢°æ’ï¼‰ã€‚
- [ ] KV å»é‡çª—å£å®¹é‡è¯„ä¼°ï¼ˆ100 ä¸‡/æ—¥ = ~1.2 ä¸‡æ¬¡ KV å†™/åˆ†é’Ÿï¼Œéœ€ç¡®è®¤é…é¢ï¼‰ã€‚
- [ ] åŒå†™æœŸé—´è®¡æ•°å¯¹æ¯”æ—¥å¿—ä¸ŠæŠ¥ä¸ç›‘æ§ã€‚

## é™„å½• G. R2 å½’æ¡£è‡ªåŠ¨åŒ–æµç¨‹

### è§¦å‘æ¡ä»¶ä¸æ—¶æœº

- **æ¯æ—¥è‡ªåŠ¨å½’æ¡£**ï¼šé€šè¿‡ Cron Triggerï¼ˆ`0 2 * * *`ï¼Œæ¯æ—¥å‡Œæ™¨ 2 ç‚¹ï¼‰è§¦å‘ã€‚
- **å½’æ¡£èŒƒå›´**ï¼š3 å¤©å‰çš„æ˜ç»†äº‹ä»¶ï¼ˆä¾‹å¦‚ä»Šå¤©æ˜¯ 10/14ï¼Œå½’æ¡£ 10/11 çš„æ•°æ®ï¼‰ã€‚
- **ç´§æ€¥å½’æ¡£**ï¼šå½“ D1 æ•°æ®åº“å¤§å°è¶…è¿‡ 800 MB æ—¶ï¼Œæ‰‹åŠ¨è§¦å‘å½’æ¡£æœ€æ—§ä¸€å¤©çš„æ•°æ®ã€‚

### å½’æ¡£ Worker å®ç°

```ts
// scheduled handler in wrangler.toml
// crons = ["0 2 * * *"]

export default {
  async scheduled(event: ScheduledEvent, env: Env, ctx: ExecutionContext) {
    await ctx.waitUntil(archiveDailyEvents(env));
  }
};

async function archiveDailyEvents(env: Env) {
  // 1. è®¡ç®—è¦å½’æ¡£çš„æ—¥æœŸï¼ˆ3 å¤©å‰ï¼‰
  const targetDate = new Date();
  targetDate.setDate(targetDate.getDate() - 3);
  const dateStr = targetDate.toISOString().split('T')[0]; // YYYY-MM-DD
  
  console.log(`å¼€å§‹å½’æ¡£ ${dateStr} çš„æ˜ç»†äº‹ä»¶`);
  
  // âš ï¸ é¿å…å†…å­˜æº¢å‡ºï¼šçœŸæ­£çš„æµå¼å¤„ç†
  // âš ï¸ R2 é™åˆ¶ï¼šMultipart Upload æ¯ä¸ªåˆ†ç‰‡å¿…é¡» â‰¥5 MiBï¼ˆæœ€åä¸€ä¸ªåˆ†ç‰‡é™¤å¤–ï¼‰
  // âš ï¸ Workers é™åˆ¶ï¼šå†…å­˜ä¸Šé™ ~128 MBï¼Œä¸èƒ½ç´¯ç§¯æ‰€æœ‰æ•°æ®
  
  const archivePath = `events-archive/${targetDate.getFullYear()}/${String(targetDate.getMonth() + 1).padStart(2, '0')}/${dateStr}.jsonl.gz`;
  
  // 2. è·å–æ€»è®°å½•æ•°
  const countResult = await env.D1.prepare(
    'SELECT COUNT(*) as count FROM traffic_events WHERE event_date = ?'
  ).bind(dateStr).first<{ count: number }>();
  
  const totalCount = countResult?.count || 0;
  
  if (totalCount === 0) {
    console.log(`${dateStr} æ— æ•°æ®ï¼Œè·³è¿‡`);
    return;
  }
  
  console.log(`å‡†å¤‡å½’æ¡£ ${totalCount} æ¡äº‹ä»¶`);
  
  // 3. å†³ç­–ä¸Šä¼ ç­–ç•¥
  const estimatedSizeBytes = totalCount * 150; // æ¯æ¡çº¦ 150 å­—èŠ‚
  const estimatedSizeGzipMB = (estimatedSizeBytes * 0.25) / (1024 * 1024); // å‹ç¼©ç‡çº¦ 75%
  
  if (estimatedSizeGzipMB < 100) {
    // ç­–ç•¥ Aï¼š<100 MBï¼Œä½¿ç”¨å•æ¬¡ put() + çœŸæ­£çš„æµå¼è¯»å–ï¼ˆä¸åœ¨å†…å­˜ç´¯ç§¯ï¼‰
    console.log(`é¢„ä¼°å¤§å° ${estimatedSizeGzipMB.toFixed(2)} MBï¼Œä½¿ç”¨å•æ¬¡ put() æµå¼ä¸Šä¼ `);
    await archiveWithSinglePut(env, dateStr, totalCount, archivePath);
  } else {
    // ç­–ç•¥ Bï¼šâ‰¥100 MBï¼Œä½¿ç”¨ Multipart Upload + ç´¯ç§¯åˆ° 5 MiB å†å‘é€
    console.log(`é¢„ä¼°å¤§å° ${estimatedSizeGzipMB.toFixed(2)} MBï¼Œä½¿ç”¨ Multipart æµå¼ä¸Šä¼ `);
    await archiveWithMultipart(env, dateStr, totalCount, archivePath);
  }
}

// ç­–ç•¥ Aï¼šçœŸæ­£çš„æµå¼ä¸Šä¼ ï¼ˆä¸åœ¨å†…å­˜ç´¯ç§¯æ•°æ®ï¼‰
async function archiveWithSinglePut(
  env: Env,
  dateStr: string,
  totalCount: number,
  archivePath: string
) {
  const BATCH_SIZE = 5000; // é™ä½æ‰¹æ¬¡å¤§å°ï¼Œå‡å°‘å†…å­˜å³°å€¼
  let offset = 0;
  let totalRecords = 0;
  
  // åˆ›å»º ReadableStreamï¼ŒæŒ‰éœ€ä» D1 è¯»å–å¹¶å‹ç¼©
  const jsonlStream = new ReadableStream({
    async pull(controller) {
      if (offset >= totalCount) {
        controller.close();
        return;
      }
      
      try {
        const { results } = await env.D1.prepare(
          'SELECT * FROM traffic_events WHERE event_date = ? ORDER BY timestamp LIMIT ? OFFSET ?'
        ).bind(dateStr, BATCH_SIZE, offset).all();
        
        if (!results || results.length === 0) {
          controller.close();
          return;
        }
        
        // é€è¡Œè¾“å‡º JSONLï¼Œç«‹å³é‡Šæ”¾å†…å­˜
        const jsonlChunk = results.map(r => JSON.stringify(r)).join('\n') + '\n';
        controller.enqueue(new TextEncoder().encode(jsonlChunk));
        
        offset += results.length;
        totalRecords += results.length;
        
        console.log(`å·²è¯»å– ${totalRecords}/${totalCount} æ¡è®°å½•`);
      } catch (error) {
        console.error('æµå¼è¯»å–å¤±è´¥:', error);
        controller.error(error);
      }
    }
  });
  
  // æµå¼å‹ç¼©å¹¶ä¸Šä¼ ï¼ˆæ•°æ®ä¸åœ¨å†…å­˜ç´¯ç§¯ï¼‰
  const gzipStream = jsonlStream.pipeThrough(new CompressionStream('gzip'));
  
  await env.R2_BUCKET.put(archivePath, gzipStream, {
    httpMetadata: {
      contentType: 'application/x-ndjson',
      contentEncoding: 'gzip'
    },
    customMetadata: {
      recordCount: totalCount.toString(),
      archiveDate: new Date().toISOString(),
      sourceDate: dateStr
    }
  });
  
  console.log(`å•æ¬¡ä¸Šä¼ å®Œæˆ: ${archivePath}, æ€»è®¡ ${totalRecords} æ¡è®°å½•`);
  
  // å½’æ¡£æˆåŠŸåæ‰§è¡Œæ¸…ç†å’Œå…ƒæ•°æ®è®°å½•
  await finishArchive(env, dateStr, totalRecords, archivePath);
}

// ç­–ç•¥ Bï¼šMultipart Upload + ç´¯ç§¯åˆ° 5 MiB å†å‘é€
async function archiveWithMultipart(
  env: Env,
  dateStr: string,
  totalCount: number,
  archivePath: string
) {
  const BATCH_SIZE = 5000;
  const MIN_PART_SIZE = 5 * 1024 * 1024; // 5 MiB
  
  let offset = 0;
  let totalRecords = 0;
  let partNumber = 1;
  let currentPartBuffer: Uint8Array[] = []; // å½“å‰åˆ†ç‰‡ç´¯ç§¯çš„æ•°æ®å—
  let currentPartSize = 0;
  
  const multipartUpload = await env.R2_BUCKET.createMultipartUpload(archivePath, {
    httpMetadata: {
      contentType: 'application/x-ndjson',
      contentEncoding: 'gzip'
    },
    customMetadata: {
      recordCount: totalCount.toString(),
      archiveDate: new Date().toISOString(),
      sourceDate: dateStr
    }
  });
  
  const uploadedParts: R2UploadedPart[] = [];
  
  try {
    while (offset < totalCount) {
      // è¯»å–ä¸€æ‰¹æ•°æ®
      const { results } = await env.D1.prepare(
        'SELECT * FROM traffic_events WHERE event_date = ? ORDER BY timestamp LIMIT ? OFFSET ?'
      ).bind(dateStr, BATCH_SIZE, offset).all();
      
      if (!results || results.length === 0) break;
      
      // è½¬æ¢ä¸º JSONL å¹¶å‹ç¼©
      const jsonlData = results.map(r => JSON.stringify(r)).join('\n') + '\n';
      const compressed = await compressGzipToUint8Array(jsonlData);
      
      // ç´¯ç§¯åˆ°å½“å‰åˆ†ç‰‡
      currentPartBuffer.push(compressed);
      currentPartSize += compressed.byteLength;
      
      offset += results.length;
      totalRecords += results.length;
      
      // æ£€æŸ¥æ˜¯å¦è¾¾åˆ° 5 MiBï¼ˆæˆ–å·²æ˜¯æœ€åä¸€æ‰¹ï¼‰
      const isLastBatch = offset >= totalCount;
      if (currentPartSize >= MIN_PART_SIZE || isLastBatch) {
        // åˆå¹¶ç¼“å†²åŒºå¹¶ä¸Šä¼ 
        const partData = concatenateUint8Arrays(currentPartBuffer);
        console.log(`ä¸Šä¼ åˆ†ç‰‡ ${partNumber}: ${(partData.byteLength / 1024 / 1024).toFixed(2)} MiB, å·²å¤„ç† ${totalRecords}/${totalCount} æ¡`);
        
        const uploadedPart = await multipartUpload.uploadPart(partNumber, partData);
        uploadedParts.push(uploadedPart);
        
        // æ¸…ç©ºç¼“å†²åŒºï¼Œé‡Šæ”¾å†…å­˜
        currentPartBuffer = [];
        currentPartSize = 0;
        partNumber++;
      }
    }
    
    // å®Œæˆ Multipart Upload
    const completed = await multipartUpload.complete(uploadedParts);
    console.log(`Multipart ä¸Šä¼ å®Œæˆ: ${archivePath}, æ€»è®¡ ${totalRecords} æ¡è®°å½•, å¤§å° ${(completed.size / 1024 / 1024).toFixed(2)} MB`);
    
    // å½’æ¡£æˆåŠŸåæ‰§è¡Œæ¸…ç†å’Œå…ƒæ•°æ®è®°å½•
    await finishArchive(env, dateStr, totalRecords, archivePath);
  } catch (error) {
    console.error('Multipart ä¸Šä¼ å¤±è´¥:', error);
    await multipartUpload.abort();
    throw error;
  }
}

// è¾…åŠ©å‡½æ•°ï¼šå‹ç¼©ä¸º Uint8Array
async function compressGzipToUint8Array(data: string): Promise<Uint8Array> {
  const stream = new ReadableStream({
    start(controller) {
      controller.enqueue(new TextEncoder().encode(data));
      controller.close();
    }
  });
  
  const compressedStream = stream.pipeThrough(new CompressionStream('gzip'));
  const reader = compressedStream.getReader();
  const chunks: Uint8Array[] = [];
  
  while (true) {
    const { done, value } = await reader.read();
    if (done) break;
    chunks.push(value);
  }
  
  return concatenateUint8Arrays(chunks);
}

// è¾…åŠ©å‡½æ•°ï¼šåˆå¹¶ Uint8Array
function concatenateUint8Arrays(arrays: Uint8Array[]): Uint8Array {
  const totalLength = arrays.reduce((sum, arr) => sum + arr.byteLength, 0);
  const result = new Uint8Array(totalLength);
  let offset = 0;
  
  for (const arr of arrays) {
    result.set(arr, offset);
    offset += arr.byteLength;
  }
  
  return result;
}

// å½’æ¡£å®Œæˆåçš„æ¸…ç†å’Œå…ƒæ•°æ®è®°å½•
async function finishArchive(
  env: Env, 
  dateStr: string, 
  totalRecords: number, 
  archivePath: string
) {
  // è®°å½•å½’æ¡£å…ƒæ•°æ®åˆ° KV
  await env.KV.put(
    `archive:metadata:${dateStr}`,
    JSON.stringify({
      path: archivePath,
      recordCount: totalRecords,
      archivedAt: new Date().toISOString(),
      format: 'jsonl.gz'
    }),
    { expirationTtl: 365 * 86400 } // ä¿ç•™ 1 å¹´
  );
  
  // åˆ é™¤ D1 ä¸­çš„è®°å½•ï¼ˆåˆ†æ‰¹åˆ é™¤ï¼Œé¿å…é•¿æ—¶é—´é”è¡¨ï¼‰
  // âš ï¸ D1ï¼ˆSQLiteï¼‰ä¸æ”¯æŒ DELETE ... LIMIT è¯­æ³•ï¼Œéœ€ä½¿ç”¨ rowid å­æŸ¥è¯¢
  let deletedTotal = 0;
  while (true) {
    const deleteResult = await env.D1.prepare(`
      DELETE FROM traffic_events 
      WHERE rowid IN (
        SELECT rowid FROM traffic_events 
        WHERE event_date = ? 
        LIMIT 5000
      )
    `).bind(dateStr).run();
    
    deletedTotal += deleteResult.meta.changes || 0;
    
    if ((deleteResult.meta.changes || 0) < 5000) break;
    
    // çŸ­æš‚ç­‰å¾…ï¼Œé¿å…æŒç»­å ç”¨æ•°æ®åº“è¿æ¥
    await new Promise(resolve => setTimeout(resolve, 100));
  }
  
  console.log(`å·²åˆ é™¤ D1 ä¸­ ${deletedTotal} æ¡è®°å½•`);
  
  // å‘é€æŒ‡æ ‡
  await env.ANALYTICS?.writeDataPoint({
    blobs: ['archive_daily', dateStr],
    doubles: [totalRecords],
    indexes: [1] // æˆåŠŸæ ‡è®°
  });
}
```

### æ¸…ç†ç­–ç•¥é…ç½®

```toml
# wrangler.toml
[triggers]
crons = [
  "0 2 * * *",    # æ¯æ—¥å½’æ¡£ï¼ˆå‡Œæ™¨ 2 ç‚¹ï¼‰
  "0 */6 * * *"   # æ¯ 6 å°æ—¶æ£€æŸ¥å®¹é‡
]

[[r2_buckets]]
binding = "R2_BUCKET"
bucket_name = "traffic-events-archive"
```

### å®¹é‡ç›‘æ§ Worker

```ts
// æ¯ 6 å°æ—¶æ‰§è¡Œ
async function checkD1Capacity(env: Env) {
  // æŸ¥è¯¢æ•°æ®åº“å¤§å°ï¼ˆSQLite pragmaï¼‰
  const { results } = await env.D1.prepare('PRAGMA page_count').all();
  const pageCount = results?.[0]?.page_count || 0;
  const dbSizeBytes = pageCount * 4096; // SQLite é»˜è®¤é¡µå¤§å° 4KB
  const dbSizeMB = dbSizeBytes / (1024 * 1024);
  
  console.log(`D1 å½“å‰å¤§å°: ${dbSizeMB.toFixed(2)} MB`);
  
  // è¶…è¿‡é˜ˆå€¼å‘Šè­¦
  if (dbSizeMB > 800) {
    console.error(`âš ï¸  D1 å®¹é‡æ¥è¿‘ä¸Šé™: ${dbSizeMB.toFixed(2)} MB / 1024 MB`);
    
    // å‘é€å‘Šè­¦ï¼ˆå¯å¯¹æ¥ Sentryã€PagerDuty ç­‰ï¼‰
    await sendAlert(env, {
      type: 'D1_CAPACITY_WARNING',
      message: `D1 å®¹é‡: ${dbSizeMB.toFixed(2)} MB`,
      threshold: 800,
      action: 'å»ºè®®æ‰‹åŠ¨è§¦å‘ç´§æ€¥å½’æ¡£'
    });
  }
  
  // è®°å½•æŒ‡æ ‡
  await env.ANALYTICS?.writeDataPoint({
    blobs: ['d1_capacity'],
    doubles: [dbSizeMB],
    indexes: [dbSizeMB > 800 ? 1 : 0]
  });
}
```

### æ‰‹åŠ¨å½’æ¡£æ¥å£ï¼ˆç®¡ç†ç«¯ï¼‰

```ts
// POST /admin/archive/trigger
app.post('/admin/archive/trigger', async (c) => {
  const { date } = await c.req.json(); // ä¾‹å¦‚ "2025-10-11"
  
  // éªŒè¯æ—¥æœŸæ ¼å¼
  if (!/^\d{4}-\d{2}-\d{2}$/.test(date)) {
    return c.json({ error: 'æ—¥æœŸæ ¼å¼é”™è¯¯' }, 400);
  }
  
  // å¼‚æ­¥è§¦å‘å½’æ¡£
  c.executionCtx.waitUntil(archiveSpecificDate(c.env, date));
  
  return c.json({ 
    message: `å·²è§¦å‘å½’æ¡£ä»»åŠ¡: ${date}`,
    status: 'pending'
  });
});
```

### æŸ¥è¯¢å½’æ¡£æ•°æ®æ¥å£

```ts
// GET /admin/archive/query?date=2025-10-11
app.get('/admin/archive/query', async (c) => {
  const date = c.req.query('date');
  
  // 1. ä» KV è¯»å–å…ƒæ•°æ®
  const metaStr = await c.env.KV.get(`archive:metadata:${date}`);
  if (!metaStr) {
    return c.json({ error: 'å½’æ¡£ä¸å­˜åœ¨' }, 404);
  }
  
  const meta = JSON.parse(metaStr);
  
  // 2. ä» R2 è¯»å–æ•°æ®
  const object = await c.env.R2_BUCKET.get(meta.path);
  if (!object) {
    return c.json({ error: 'R2 æ–‡ä»¶ä¸¢å¤±' }, 500);
  }
  
  // 3. è§£å‹ç¼©å¹¶è¿”å›
  const compressed = await object.arrayBuffer();
  const decompressed = await decompressGzip(compressed);
  const events = JSON.parse(decompressed);
  
  return c.json({
    date,
    recordCount: events.length,
    archivedAt: meta.archivedAt,
    events
  });
});
```

### å½’æ¡£æ•°æ®ç”Ÿå‘½å‘¨æœŸ

| é˜¶æ®µ | ä½ç½® | ä¿ç•™æœŸé™ | è®¿é—®é¢‘ç‡ |
|------|------|----------|----------|
| å®æ—¶ | D1 æ˜ç»† | 3 å¤© | é«˜ |
| è¿‘æœŸ | R2ï¼ˆJSON.gzï¼‰ | 90 å¤© | ä¸­ |
| å†å² | R2ï¼ˆParquetï¼‰ | 1 å¹´æˆ–æŒ‰åˆè§„ | ä½ |
| å½’æ¡£å…ƒæ•°æ® | KV | 1 å¹´ | ä½ |

### æˆæœ¬ä¼˜åŒ–å»ºè®®

1. **å‹ç¼©ç‡**ï¼šJSON.gz å¯è¾¾åˆ° 80% å‹ç¼©ç‡ï¼ŒParquet æ ¼å¼æ›´ä¼˜ï¼ˆ90%+ï¼‰ã€‚
2. **R2 Class B æ“ä½œ**ï¼šæ¯æ—¥å½’æ¡£ 1 æ¬¡ï¼ˆPUTï¼‰+ å¶å°”æŸ¥è¯¢ï¼ˆGETï¼‰ï¼Œæˆæœ¬å¯å¿½ç•¥ã€‚
3. **å®šæœŸæ¸…ç†**ï¼šè¶…è¿‡ 1 å¹´çš„ R2 æ–‡ä»¶å¯é€šè¿‡ Lifecycle Policy è‡ªåŠ¨åˆ é™¤æˆ–è½¬ç§»è‡³ Glacierã€‚

### å›æ»šä¸æ¢å¤

- è‹¥å½’æ¡£åå‘ç°æ•°æ®é—®é¢˜ï¼Œå¯é€šè¿‡ `/admin/archive/query` è¯»å– R2 æ•°æ®ï¼Œé‡æ–°å¯¼å…¥ D1ã€‚
- ä¿ç•™å½’æ¡£å…ƒæ•°æ®ï¼ˆKVï¼‰è‡³å°‘ 1 å¹´ï¼Œä¾¿äºè¿½æº¯ä¸å®¡è®¡ã€‚

---

## é™„å½• H. æŠ€æœ¯å®¡æŸ¥é—®é¢˜è§£ç­”

### Q1: D1 ä¸æ”¯æŒè‡ªå®šä¹‰èšåˆå‡½æ•°ï¼Œå¦‚ä½•å®ç° t-digest/HLL åˆå¹¶ï¼Ÿ

**å·²ä¿®æ­£**ï¼š

- åŸæ–¹æ¡ˆåœ¨ SQL ä¸­ä½¿ç”¨ `merge_tdigest()` / `merge_hll()` è‡ªå®šä¹‰å‡½æ•°ï¼ŒD1ï¼ˆSQLiteï¼‰ä¸æ”¯æŒ UDFã€‚
- **æ–°æ–¹æ¡ˆ**ï¼šåœ¨ Aggregator Worker ä¸­å®Œæˆç»Ÿè®¡åˆå¹¶ï¼ˆè§ 5.2 èŠ‚ä¸é™„å½• Gï¼‰ï¼š
  1. ä» D1 è¯»å–å½“å‰å°æ—¶æ¡¶çš„ BLOB æ•°æ®ã€‚
  2. åœ¨ Worker å†…å­˜ä¸­ååºåˆ—åŒ–ï¼Œä½¿ç”¨ npm åŒ…ï¼ˆ`tdigest`ã€`hyperloglog`ï¼‰å®Œæˆå¢é‡åˆå¹¶ã€‚
  3. åºåˆ—åŒ–åå†™å› D1ï¼ŒSQL ä»…åšç®€å• upsertã€‚
- **ä¾èµ–åº“**ï¼š
  - `tdigest`: è½»é‡çº§ t-digest å®ç°ï¼Œæ”¯æŒ p50/p95/p99 è®¡ç®—ã€‚
  - `hyperloglog`: HLL å®ç°ï¼Œç”¨äº unique IP ä¼°ç®—ã€‚
- **æ€§èƒ½è¯„ä¼°**ï¼šå•æ‰¹æ¬¡ï¼ˆ100 æ¡äº‹ä»¶ï¼‰å†…å­˜åˆå¹¶è€—æ—¶ < 10msã€‚

### Q2: 100 ä¸‡è¯·æ±‚/æ—¥ä¼šåœ¨ä¸åˆ°ä¸€å‘¨å¡«æ»¡ D1 1GBï¼Œå¦‚ä½•ç®¡ç†å®¹é‡ï¼Ÿ

**å·²è¡¥å……**ï¼ˆè§ 5.3 èŠ‚ã€é™„å½• Gï¼‰ï¼š

1. **å¼ºåˆ¶ä¿ç•™æœŸé™**ï¼š
   - æ˜ç»†äº‹ä»¶ï¼š**3 å¤©**ï¼ˆä¿ç•™æœ€è¿‘ 3 å¤©ï¼Œçº¦ 450 MBï¼‰ã€‚
   - èšåˆæ•°æ®ï¼š**90 å¤©**ï¼ˆå ç”¨ç©ºé—´ < 50 MBï¼‰ã€‚

2. **è‡ªåŠ¨åŒ–å½’æ¡£æµç¨‹**ï¼š
   - æ¯æ—¥å‡Œæ™¨ 2 ç‚¹ï¼ˆCron Triggerï¼‰è‡ªåŠ¨å½’æ¡£ 3 å¤©å‰çš„æ•°æ®è‡³ R2ã€‚
   - å½’æ¡£æ ¼å¼ï¼šJSON.gzï¼ˆPhase 2ï¼‰ï¼ŒParquetï¼ˆPhase 5 ä¼˜åŒ–ï¼‰ã€‚
   - å½’æ¡£è·¯å¾„ï¼š`events-archive/YYYY/MM/YYYY-MM-DD.json.gz`ã€‚

3. **å®¹é‡ç›‘æ§**ï¼š
   - æ¯ 6 å°æ—¶æŸ¥è¯¢ D1 å¤§å°ï¼ˆ`PRAGMA page_count`ï¼‰ã€‚
   - è¶…è¿‡ 800 MB è§¦å‘å‘Šè­¦ï¼Œå»ºè®®æ‰‹åŠ¨å½’æ¡£ã€‚

4. **æ¸…ç†æœºåˆ¶**ï¼š
   - å½’æ¡£éªŒè¯æˆåŠŸåï¼Œä» D1 åˆ é™¤å¯¹åº”æ—¥æœŸçš„æ˜ç»†è®°å½•ã€‚
   - R2 å½’æ¡£æ•°æ®ä¿ç•™ 1 å¹´æˆ–æŒ‰åˆè§„è¦æ±‚ã€‚

5. **ç´§æ€¥é¢„æ¡ˆ**ï¼š
   - æä¾›æ‰‹åŠ¨å½’æ¡£æ¥å£ï¼š`POST /admin/archive/trigger`ã€‚
   - è‹¥ D1 æ»¡è½½ï¼Œä¸´æ—¶æé«˜é‡‡æ ·ç‡ï¼ˆé™ä½å†™å…¥é‡ï¼‰ã€‚

### Q3: Phase 1 ä¸ºä½•ä»è®© Worker å†™ PathCollector DOï¼Œè€Œä¸æ˜¯ç›´æ¥å†™ Queueï¼Ÿ

**å·²è°ƒæ•´**ï¼ˆè§ 5.1 èŠ‚ã€Phase 1 è®¡åˆ’ï¼‰ï¼š

- **æ–°æ–¹æ¡ˆ**ï¼š**Worker ç›´æ¥å†™ Workers Queue**ï¼Œè·³è¿‡ DO è½¬å‘ï¼Œé¿å…é¢å¤–å¾€è¿”ã€‚
- **æ—§ DO ä¿ç•™ç­–ç•¥**ï¼š
  - Phase 1~2 æœŸé—´ï¼Œæ—§ PathCollector DO ç»§ç»­è¿è¡Œèšåˆé€»è¾‘ï¼Œä½œä¸º**å…œåº•è¯»è·¯å¾„**ã€‚
  - æ¥å£å±‚åŒæ—¶è¯»å–æ–°æ—§æ•°æ®æºï¼Œå¯¹æ¯”è®¡æ•°ä¸€è‡´æ€§ã€‚
  - Phase 3 ç°åº¦åˆ‡æ¢åï¼Œé€æ­¥ä¸‹çº¿ DOã€‚
- **åŒå†™æœŸé—´å»é‡**ï¼ˆè§é™„å½• Fï¼‰ï¼š
  - Worker ç”Ÿæˆå¹‚ç­‰ IDï¼ˆåŸºäº `crypto.subtle.digest`ï¼‰ã€‚
  - Aggregator ä½¿ç”¨ KV æ»‘åŠ¨çª—å£å»é‡ï¼ˆ1 å°æ—¶çª—å£ï¼‰ã€‚
  - å¯é€‰ï¼šé‡‡æ ·éªŒè¯ï¼ˆ1% æµé‡åŒæ—¶å†™ DO + Queueï¼ŒéªŒè¯è®¡æ•°ä¸€è‡´ï¼‰ã€‚

### Q4: å¦‚ä½•é¿å…åŒå†™æœŸé—´é‡å¤è®¡æ•°ï¼Ÿ

**å·²è¡¥å……**ï¼ˆè§é™„å½• Fï¼‰ï¼š

1. **å¹‚ç­‰ ID ç”Ÿæˆ**ï¼š
   ```ts
   // æ ¼å¼ï¼š1730956800000-a1b2c3d4
   const id = `${timestamp}-${hash(clientIP + path + requestId).slice(0, 8)}`;
   ```

2. **Aggregator ç«¯å»é‡**ï¼š
   ```ts
   // KV å­˜å‚¨æœ€è¿‘ 1 å°æ—¶å·²å¤„ç† ID
   const dedupKey = `dedup:${event.id}`;
   const exists = await env.KV.get(dedupKey);
   if (exists) return; // è·³è¿‡é‡å¤äº‹ä»¶
   await env.KV.put(dedupKey, '1', { expirationTtl: 3600 });
   ```

3. **å¹¶è¡ŒéªŒè¯**ï¼š
   - Phase 1 åœ¨æ¥å£å±‚åŒæ—¶è¯»å– D1ï¼ˆæ–°ï¼‰å’Œ DOï¼ˆæ—§ï¼‰çš„ç»Ÿè®¡æ•°æ®ã€‚
   - è®°å½•è®¡æ•°å·®å¼‚åˆ°æ—¥å¿—ï¼Œè¶…è¿‡é˜ˆå€¼ï¼ˆå¦‚ 100ï¼‰è§¦å‘å‘Šè­¦ã€‚

4. **é™çº§ç­–ç•¥**ï¼š
   - è‹¥ Queue æ¶ˆè´¹å¼‚å¸¸ï¼Œè‡ªåŠ¨ fallback åˆ° DO æ—§è·¯å¾„ã€‚
   - ä¿ç•™ `?source=do` å‚æ•°å¼ºåˆ¶è¯»å–æ—§æ•°æ®ä½œå¯¹æ¯”ã€‚

### Q5: å¹‚ç­‰ ID ç”Ÿæˆæ˜¯å¦åœ¨ç°æœ‰ Worker ä¸­å…·å¤‡å®ç°æ¡ä»¶ï¼Ÿ

**å·²éªŒè¯**ï¼š

- Worker è¿è¡Œæ—¶æ”¯æŒ `crypto.subtle.digest`ï¼ˆWeb Crypto APIï¼‰ã€‚
- ç°æœ‰ä»£ç å·²æœ‰ `requestId` ç”Ÿæˆèƒ½åŠ›ï¼ˆé€šå¸¸é€šè¿‡ `crypto.randomUUID()`ï¼‰ã€‚
- æ€§èƒ½æµ‹è¯•ç›®æ ‡ï¼šå•æ¬¡ SHA-256 å“ˆå¸Œ < 1msï¼ˆé¢„è®¡ 0.1~0.5msï¼‰ã€‚
- **å‰ç½®äº‹é¡¹**ï¼ˆè§ 9 èŠ‚ï¼‰ï¼š
  - éªŒè¯ `crypto.subtle.digest` æ€§èƒ½ä¸å”¯ä¸€æ€§ï¼ˆ100 ä¸‡æ ·æœ¬æ— ç¢°æ’ï¼‰ã€‚
  - è¯„ä¼° KV å»é‡çª—å£å®¹é‡ï¼ˆ~1.2 ä¸‡æ¬¡å†™/åˆ†é’Ÿï¼‰ã€‚

### Q6: æ˜ç»†ä¿ç•™æœŸé™ã€å¤šåº“åˆ‡åˆ†æˆ–å‘¨æœŸæ€§å½’æ¡£/åˆ é™¤çš„å…·ä½“æ–¹æ¡ˆï¼Ÿ

**å·²æ˜ç¡®**ï¼ˆè§ 5.3 èŠ‚ã€é™„å½• Gï¼‰ï¼š

| ç­–ç•¥ | è§¦å‘æ¡ä»¶ | æ‰§è¡ŒåŠ¨ä½œ | è‡ªåŠ¨åŒ– |
|------|----------|----------|--------|
| æ¯æ—¥å½’æ¡£ | æ¯æ—¥å‡Œæ™¨ 2 ç‚¹ | å½’æ¡£ 3 å¤©å‰æ•°æ®è‡³ R2 | Cron Trigger |
| æ¯æ—¥æ¸…ç† | å½’æ¡£æˆåŠŸå | åˆ é™¤ D1 ä¸­å¯¹åº”è®°å½• | è‡ªåŠ¨ |
| å®¹é‡æ£€æŸ¥ | æ¯ 6 å°æ—¶ | æ£€æŸ¥ D1 å¤§å°ï¼Œè¶… 800MB å‘Šè­¦ | Cron Trigger |
| ç´§æ€¥å½’æ¡£ | æ‰‹åŠ¨è§¦å‘æˆ–å‘Šè­¦å | å½’æ¡£æœ€æ—§ä¸€å¤©æ•°æ® | ç®¡ç†æ¥å£ |
| R2 æ¸…ç† | è¶…è¿‡ 1 å¹´ | åˆ é™¤æˆ–è½¬ç§»è‡³ Glacier | Lifecycle Policy |

**å¤šåº“åˆ‡åˆ†ï¼ˆPhase 5 å¯é€‰ï¼‰**ï¼š

- è‹¥å•åº“å‹åŠ›ä»å¤§ï¼Œå¯æŒ‰æœˆæˆ–æŒ‰è·¯å¾„å‰ç¼€ï¼ˆhash åˆ†æ¡¶ï¼‰æ‹†åˆ†ä¸ºå¤šä¸ª D1ã€‚
- æŸ¥è¯¢å±‚é€šè¿‡è·¯ç”±é€»è¾‘åˆ†å‘ï¼Œèšåˆæ—¶è”åˆå¤šåº“ç»“æœã€‚

### Q7: å¤šæ¶ˆè´¹è€…å¹¶å‘å†™å…¥åŒä¸€ (path, hour_bucket) æ—¶ä¼šä¸¢å¤±å¢é‡ï¼Œå¦‚ä½•è§£å†³ï¼Ÿ

**å·²ä¿®æ­£**ï¼ˆè§ 5.3 èŠ‚èšåˆå†™å…¥æµç¨‹ï¼‰ï¼š

**é—®é¢˜æ ¹å› **ï¼šåŸæ–¹æ¡ˆä½¿ç”¨ read-modify-write æ¨¡å¼ï¼Œå¤šæ¶ˆè´¹è€…åŸºäºåŒä¸€æ—§å€¼å¹¶è¡Œæ›´æ–°æ—¶ï¼Œåå†™å…¥è€…ä¼šè¦†ç›–å‰è€…çš„ BLOB æ”¹åŠ¨ï¼ˆlost updateï¼‰ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. **æ–¹æ¡ˆ Aï¼šDurable Object èšåˆåè°ƒå™¨ï¼ˆæ¨èç”¨äºé«˜æµé‡ï¼‰**
   - Queue Consumer æŒ‰ `${path}:${hourBucket}` åˆ†ç»„äº‹ä»¶ã€‚
   - æ¯ä¸ª group å¯¹åº”ä¸€ä¸ª Aggregator DO å®ä¾‹ï¼ˆé€šè¿‡ `idFromName` ç¡®ä¿è·¯ç”±ä¸€è‡´æ€§ï¼‰ã€‚
   - DO å†…ä¸²è¡Œæ‰§è¡Œ read-merge-writeï¼Œé¿å…å¹¶å‘å†²çªã€‚
   - DO å®šæœŸï¼ˆå¦‚æ¯ 10 æ¬¡æ›´æ–°ï¼‰åˆ·æ–° KV å¿«ç…§ï¼Œå‡å°‘ KV å†™å…¥é¢‘ç‡ã€‚
   - **ä¼˜åŠ¿**ï¼šå¯æ‰©å±•ï¼ˆæ•°åƒä¸ª path+hour å¹¶è¡Œå¤„ç†ï¼‰ã€ä½å»¶è¿Ÿã€‚
   - **æˆæœ¬**ï¼šæ¯ DO å®ä¾‹ ~$0.000005/è¯·æ±‚ï¼Œ100ä¸‡/æ—¥çº¦ $5/æœˆã€‚

2. **æ–¹æ¡ˆ Bï¼šå•æ¶ˆè´¹è€…ä¸²è¡Œå¤„ç†ï¼ˆæ¨èç”¨äºåˆæœŸéªŒè¯ï¼‰**
   - åœ¨ `wrangler.toml` ä¸­è®¾ç½® `max_concurrency = 1`ï¼Œå¼ºåˆ¶å•æ¶ˆè´¹è€…ã€‚
   - ååé‡çº¦ 5~10 ä¸‡äº‹ä»¶/åˆ†é’Ÿï¼Œè¶³å¤Ÿæ”¯æ’‘ 50 ä¸‡/æ—¥æµé‡ã€‚
   - **ä¼˜åŠ¿**ï¼šå®ç°ç®€å•ã€æ— å¹¶å‘é—®é¢˜ã€æˆæœ¬ä½ã€‚
   - **åŠ£åŠ¿**ï¼šå•ç‚¹æ•…éšœã€æ— æ³•æ°´å¹³æ‰©å±•ã€‚

**æ¨èç­–ç•¥**ï¼š
- **Phase 1~2**ï¼šä½¿ç”¨å•æ¶ˆè´¹è€…ï¼Œå¿«é€ŸéªŒè¯ç«¯åˆ°ç«¯æµç¨‹ã€‚
- **Phase 4~5**ï¼šè¿ç§»åˆ° DO åè°ƒå™¨ï¼Œæ”¯æŒæ°´å¹³æ‰©å±•è‡³ç™¾ä¸‡çº§ã€‚

**å¥åº·ç›‘æ§**ï¼ˆå•æ¶ˆè´¹è€…åœºæ™¯ï¼‰ï¼š
- ç›‘æ§é˜Ÿåˆ—ç§¯å‹ï¼šè¶…è¿‡ 5 ä¸‡æ¡è§¦å‘å‘Šè­¦ã€‚
- ç›‘æ§æ¶ˆè´¹å»¶è¿Ÿï¼šè¶…è¿‡ 5 åˆ†é’Ÿè§¦å‘å‘Šè­¦ã€‚
- å®ç°å¿ƒè·³æ£€æµ‹ï¼šæ¶ˆè´¹è€…æ¯åˆ†é’Ÿæ›´æ–° KV å¿ƒè·³æ—¶é—´æˆ³ï¼Œè¶…è¿‡ 3 åˆ†é’Ÿæœªæ›´æ–°åˆ™å‘Šè­¦ã€‚

### Q8: å½’æ¡£æµç¨‹ä¸€æ¬¡æ€§ SELECT * ä¼šå¯¼è‡´ OOM/è¶…æ—¶ï¼Œå¦‚ä½•å¤„ç†ï¼Ÿ

**å·²ä¿®æ­£**ï¼ˆè§é™„å½• G å½’æ¡£ Worker å®ç°ï¼‰ï¼š

**é—®é¢˜æ ¹å› **ï¼š100 ä¸‡æ¡è®°å½•çº¦ 150 MBï¼Œè¿œè¶… Workers 128 MB å†…å­˜é™åˆ¶ï¼›å•æ¬¡æŸ¥è¯¢å’Œ JSON.stringify ä¹Ÿä¼šè¶…è¿‡ CPU æ—¶é—´é¢„ç®—ï¼ˆ30ç§’ï¼‰ã€‚

**è§£å†³æ–¹æ¡ˆï¼šåˆ†æ‰¹å¤„ç† + R2 Multipart Upload**

1. **åˆ†é¡µæŸ¥è¯¢**ï¼š
   ```sql
   SELECT * FROM traffic_events 
   WHERE event_date = ? 
   ORDER BY timestamp 
   LIMIT 10000 OFFSET ?
   ```
   - æ¯æ‰¹ 1 ä¸‡æ¡ï¼Œçº¦ 1.5 MBï¼Œå®‰å…¨åœ¨å†…å­˜é™åˆ¶å†…ã€‚

2. **æµå¼å†™å…¥ R2**ï¼š
   - ä½¿ç”¨ `R2Bucket.createMultipartUpload()` APIã€‚
   - æ¯æ‰¹æ•°æ®ç‹¬ç«‹å‹ç¼©å¹¶ä¸Šä¼ ä¸ºä¸€ä¸ª partã€‚
   - æ‰€æœ‰ part ä¸Šä¼ å®Œæˆåè°ƒç”¨ `complete()`ã€‚
   - å¤±è´¥æ—¶è°ƒç”¨ `abort()` å›æ»šï¼Œé¿å…äº§ç”Ÿåƒåœ¾æ–‡ä»¶ã€‚

3. **æ ¼å¼ä¼˜åŒ–**ï¼š
   - ä½¿ç”¨ JSONLï¼ˆJSON Linesï¼‰è€Œéå•ä¸ªå¤§ JSON æ•°ç»„ã€‚
   - ä¾¿äºæµå¼å¤„ç†å’Œåç»­å¢é‡è¯»å–ã€‚
   - æ ¼å¼ï¼š`events-archive/2025/10/2025-10-14.jsonl.gz`ã€‚

4. **åˆ†æ‰¹åˆ é™¤**ï¼š
   - å½’æ¡£æˆåŠŸååˆ†æ‰¹åˆ é™¤ D1 è®°å½•ï¼ˆæ¯æ‰¹ 5000 æ¡ï¼‰ã€‚
   - é¿å…é•¿æ—¶é—´é”è¡¨å½±å“å…¶ä»–æŸ¥è¯¢ã€‚

**æ€§èƒ½è¯„ä¼°**ï¼š
- 100 ä¸‡æ¡è®°å½• â†’ 100 ä¸ªæ‰¹æ¬¡ â†’ æ¯æ‰¹å¤„ç†çº¦ 2 ç§’ â†’ æ€»è€—æ—¶ ~3.5 åˆ†é’Ÿã€‚
- å•ä¸ª Cron æ‰§è¡Œæ—¶é—´é™åˆ¶ï¼ˆ10 åˆ†é’Ÿï¼‰è¶³å¤Ÿå®Œæˆã€‚

**å¤‡é€‰æ–¹æ¡ˆï¼ˆPhase 5ï¼‰**ï¼š
- ä½¿ç”¨ Workers Analytics Engine ä½œä¸ºä¸­è½¬ï¼š
  - å®æ—¶å†™å…¥ Analytics Engineï¼ˆé‡‡æ · 100%ï¼‰ã€‚
  - é€šè¿‡ SQL API æ‰¹é‡å¯¼å‡ºåˆ° R2ã€‚
  - ä¼˜åŠ¿ï¼šåŸç”Ÿæ”¯æŒå¤§æ•°æ®å¯¼å‡ºï¼Œæ— éœ€æ‰‹åŠ¨åˆ†é¡µã€‚
  - æˆæœ¬ï¼š~$0.25/ç™¾ä¸‡äº‹ä»¶ã€‚

### Q9: å¦‚ä½•é¿å… DO è°ƒç”¨è¶…è¿‡ Workers 50 subrequest é™åˆ¶ï¼Ÿ

**é—®é¢˜æ ¹å› **ï¼šå½“æ‰¹æ¬¡åŒ…å« >50 ä¸ªä¸åŒçš„ `(path, hour)` ç»„åˆæ—¶ï¼Œ`Promise.all` ä¼šåŒæ—¶å‘èµ· >50 ä¸ª DO fetch è¯·æ±‚ï¼Œè¶…å‡º Workers å•æ¬¡æ‰§è¡Œçš„ subrequest ä¸Šé™ï¼ˆ50 ä¸ªï¼‰ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼ˆå·²å®æ–½ï¼‰ï¼š

1. **åˆ†å—å¤„ç†**ï¼š
   ```ts
   const CHUNK_SIZE = 45; // ç•™ 5 ä¸ªä½™é‡ç»™å…¶ä»–è¯·æ±‚ï¼ˆå¦‚ D1ã€KVï¼‰
   const groupEntries = Array.from(groups.entries());
   
   for (let i = 0; i < groupEntries.length; i += CHUNK_SIZE) {
     const chunk = groupEntries.slice(i, i + CHUNK_SIZE);
     const promises = chunk.map(/* DO fetch */);
     await Promise.allSettled(promises); // ä½¿ç”¨ allSettled å®¹é”™
   }
   ```

2. **å®¹é”™å¤„ç†**ï¼š
   - ä½¿ç”¨ `Promise.allSettled` è€Œé `Promise.all`ï¼Œå•ä¸ª DO å¤±è´¥ä¸å½±å“å…¶ä»–ã€‚
   - è®°å½•å¤±è´¥çš„ key åˆ°æ—¥å¿—ï¼Œä¾¿äºåç»­é‡è¯•æˆ–å‘Šè­¦ã€‚

3. **å¤‡é€‰æ–¹æ¡ˆï¼ˆè‹¥ key è¿‡å¤šï¼‰**ï¼š
   - æ”¹ä¸º"æ‰¹é‡èšåˆ DO"ï¼šå•ä¸ª DO æ¥æ”¶ `[{key, events}]` æ•°ç»„ï¼Œå†…éƒ¨å¾ªç¯å¤„ç†ã€‚
   - ä¼˜åŠ¿ï¼šåªéœ€ 1 ä¸ª subrequestï¼Œç¼ºç‚¹æ˜¯å•ä¸ª DO å¤„ç†æ—¶é—´å˜é•¿ã€‚

**æ€§èƒ½å½±å“**ï¼š
- 100 ä¸ª key â†’ éœ€ 3 æ‰¹ï¼ˆ45 + 45 + 10ï¼‰ï¼Œæ¯æ‰¹å¹¶è¡Œæ‰§è¡Œï¼Œæ€»å»¶è¿Ÿçº¦ 200-300msã€‚
- è¿œä¼˜äºä¸²è¡Œå¤„ç†ï¼ˆ5 ç§’+ï¼‰ã€‚

### Q10: R2 Multipart Upload æœ€å°åˆ†ç‰‡å¤§å°é—®é¢˜

**é—®é¢˜æ ¹å› **ï¼šåŸæ–¹æ¡ˆ `BATCH_SIZE = 10000`ï¼ˆçº¦ 1.5 MBï¼‰è¿œä½äº R2/S3 è¦æ±‚çš„ 5 MiB æœ€å°åˆ†ç‰‡å¤§å°ï¼ˆæœ€åä¸€ä¸ªåˆ†ç‰‡é™¤å¤–ï¼‰ï¼Œå¯¼è‡´ `uploadPart` å¤±è´¥ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼ˆå·²å®æ–½ï¼‰ï¼š

1. **è°ƒæ•´æ‰¹æ¬¡å¤§å°**ï¼š
   ```ts
   const BATCH_SIZE = 35000; // çº¦ 5.3 MBï¼ˆgzip å‰ï¼‰ï¼Œå‹ç¼©å â‰¥5 MiB
   ```

2. **éªŒè¯æ–¹æ¡ˆ**ï¼š
   - æŒ‰æ¯æ¡äº‹ä»¶ 150 å­—èŠ‚è®¡ç®—ï¼š35000 Ã— 150 = 5.25 MBï¼ˆgzip å‰ï¼‰ã€‚
   - JSON.gz å‹ç¼©ç‡çº¦ 20-30%ï¼ˆå®é™…å¤§å° 1-1.5 MB gzip åï¼‰ã€‚
   - âš ï¸ **é—®é¢˜**ï¼šgzip åå¯èƒ½ä» <5 MiBï¼Œéœ€è¿›ä¸€æ­¥è°ƒæ•´ã€‚

3. **æœ€ç»ˆæ–¹æ¡ˆ**ï¼ˆå·²å®æ–½ï¼‰ï¼š
   
   **æ–¹æ¡ˆ Aï¼ˆæ¨èç”¨äº <100 MB åœºæ™¯ï¼‰**ï¼šçœŸæ­£çš„æµå¼ `put()`
   - ä½¿ç”¨ `ReadableStream` çš„ `pull()` æ–¹æ³•ï¼ŒæŒ‰éœ€ä» D1 è¯»å–æ•°æ®
   - æ¯æ¬¡åªä¿ç•™å½“å‰æ‰¹æ¬¡ï¼ˆ5000 æ¡ï¼‰åœ¨å†…å­˜ä¸­
   - é€šè¿‡ `pipeThrough(CompressionStream)` æµå¼å‹ç¼©
   - **ä¸åœ¨å†…å­˜ä¸­ç´¯ç§¯æ•°æ®**ï¼Œé¿å… OOM
   - é€‚ç”¨åœºæ™¯ï¼š100 ä¸‡æ¡/æ—¥ â‰ˆ gzip å 30-45 MB
   
   **æ–¹æ¡ˆ Bï¼ˆç”¨äº â‰¥100 MB åœºæ™¯ï¼‰**ï¼šMultipart Upload + ç´¯ç§¯åˆ° 5 MiB
   - åˆ†æ‰¹è¯»å– D1ï¼ˆ5000 æ¡/æ‰¹ï¼‰
   - å‹ç¼©åç´¯ç§¯åˆ° `currentPartBuffer`ï¼ˆUint8Array æ•°ç»„ï¼‰
   - å½“ç´¯ç§¯å¤§å° â‰¥5 MiB æ—¶ï¼Œåˆå¹¶å¹¶ä¸Šä¼ ä¸ºä¸€ä¸ª part
   - ä¸Šä¼ å**ç«‹å³æ¸…ç©ºç¼“å†²åŒº**ï¼Œé‡Šæ”¾å†…å­˜
   - ç¡®ä¿æ¯ä¸ª partï¼ˆé™¤æœ€åä¸€ä¸ªï¼‰â‰¥5 MiB
   - å†…å­˜å³°å€¼ï¼šå•ä¸ª part çš„å¤§å°ï¼ˆçº¦ 5-10 MBï¼‰ï¼Œå®‰å…¨åœ¨ 128 MB é™åˆ¶å†…
   
   **è‡ªåŠ¨å†³ç­–é€»è¾‘**ï¼š
   ```ts
   const estimatedSizeGzipMB = (totalCount * 150 * 0.25) / (1024 * 1024);
   if (estimatedSizeGzipMB < 100) {
     await archiveWithSinglePut(); // çœŸæ­£çš„æµå¼ä¸Šä¼ 
   } else {
     await archiveWithMultipart(); // ç´¯ç§¯åˆ° 5 MiB å†å‘é€
   }
   ```

**æ¨èç­–ç•¥**ï¼š
- 100 ä¸‡æ¡/æ—¥ â†’ æ–¹æ¡ˆ Aï¼ˆæµå¼å•æ¬¡ `put()`ï¼‰
- 400 ä¸‡æ¡/æ—¥+ â†’ æ–¹æ¡ˆ Bï¼ˆMultipartï¼Œç´¯ç§¯åˆ° 5 MiBï¼‰
- Phase 2~3 ä½¿ç”¨æ–¹æ¡ˆ Aï¼ŒPhase 4+ æ ¹æ®å®é™…æµé‡åˆ‡æ¢

### Q11: D1 ä¸æ”¯æŒ DELETE ... LIMIT è¯­æ³•

**é—®é¢˜æ ¹å› **ï¼šD1 ä½¿ç”¨çš„ SQLite ç¼–è¯‘é…ç½®æœªå¯ç”¨ `SQLITE_ENABLE_UPDATE_DELETE_LIMIT` é€‰é¡¹ï¼Œç›´æ¥ä½¿ç”¨ `DELETE ... LIMIT` ä¼šæŠ¥è¯­æ³•é”™è¯¯ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼ˆå·²å®æ–½ï¼‰ï¼š

ä½¿ç”¨ `rowid` å­æŸ¥è¯¢ï¼š

```sql
-- âŒ é”™è¯¯ï¼ˆD1 ä¸æ”¯æŒï¼‰
DELETE FROM traffic_events WHERE event_date = ? LIMIT 5000;

-- âœ… æ­£ç¡®
DELETE FROM traffic_events 
WHERE rowid IN (
  SELECT rowid FROM traffic_events 
  WHERE event_date = ? 
  LIMIT 5000
);
```

**æ€§èƒ½è€ƒé‡**ï¼š
- å­æŸ¥è¯¢ `SELECT rowid ... LIMIT` ä¼šå…ˆç”Ÿæˆä¸´æ—¶ç»“æœé›†ï¼ˆ5000 è¡Œï¼‰ï¼Œç„¶åæ‰¹é‡åˆ é™¤ã€‚
- æ€§èƒ½ä¸ç›´æ¥ `DELETE LIMIT` åŸºæœ¬ç›¸åŒï¼ˆéƒ½éœ€æ‰«æ 5000 è¡Œï¼‰ã€‚
- æ·»åŠ ç´¢å¼• `CREATE INDEX idx_events_date ON traffic_events(event_date)` åŠ é€ŸæŸ¥è¯¢ã€‚

**å¤‡é€‰æ–¹æ¡ˆ**ï¼ˆè‹¥æ€§èƒ½ä¸è¶³ï¼‰ï¼š
- æŒ‰ä¸»é”®èŒƒå›´åˆ é™¤ï¼š
  ```sql
  DELETE FROM traffic_events 
  WHERE id >= ? AND id < ? 
  AND event_date = ?;
  ```
  - éœ€å…ˆæŸ¥è¯¢ `MIN(id)` å’Œ `MAX(id)`ï¼Œç„¶åæŒ‰èŒƒå›´åˆ†æ‰¹åˆ é™¤ã€‚

### Q12: KV åˆ·æ–°è®¡æ•°é€»è¾‘é”™è¯¯

**é—®é¢˜æ ¹å› **ï¼šåŸä»£ç åœ¨é€’å¢å‰åˆ¤æ–­ `updateCount % 10 === 0`ï¼Œå¯¼è‡´ï¼š
- ç¬¬ 1 æ¬¡æ›´æ–°ï¼š`updateCount = 0 â†’ 0 % 10 === 0 â†’ è§¦å‘åˆ·æ–°` âŒ
- ç¬¬ 10 æ¬¡æ›´æ–°ï¼š`updateCount = 9 â†’ 9 % 10 !== 0 â†’ ä¸åˆ·æ–°` âŒ

**è§£å†³æ–¹æ¡ˆ**ï¼ˆå·²å®æ–½ï¼‰ï¼š

```ts
// âŒ é”™è¯¯
const updateCount = (await storage.get('updateCount')) || 0;
await storage.put('updateCount', updateCount + 1);
if (updateCount % 10 === 0) { /* åˆ·æ–° */ }

// âœ… æ­£ç¡®
const updateCount = (await storage.get('updateCount')) || 0;
const nextCount = updateCount + 1;
await storage.put('updateCount', nextCount);
if (nextCount % 10 === 0) { /* åˆ·æ–° */ }
```

**é¢„æœŸè¡Œä¸º**ï¼š
- ç¬¬ 1~9 æ¬¡æ›´æ–°ï¼šä¸åˆ·æ–°ã€‚
- ç¬¬ 10 æ¬¡æ›´æ–°ï¼š`nextCount = 10 â†’ è§¦å‘åˆ·æ–°` âœ…
- ç¬¬ 11~19 æ¬¡ï¼šä¸åˆ·æ–°ã€‚
- ç¬¬ 20 æ¬¡ï¼šå†æ¬¡åˆ·æ–°ã€‚

### Q13: npm åº“ Workers å…¼å®¹æ€§é£é™©

**é—®é¢˜æ ¹å› **ï¼šè®¸å¤šæµè¡Œçš„ t-digest/HLL npm åŒ…ä¾èµ– Node.js ç‰¹å®š APIï¼ˆå¦‚ `Buffer`ã€`fs`ã€`crypto` æ¨¡å—ï¼‰ï¼Œåœ¨ Workers è¿è¡Œæ—¶ä¼šå¤±è´¥ã€‚

**éªŒè¯æ¸…å•**ï¼ˆPhase 0 å¿…é¡»å®Œæˆï¼‰ï¼š

1. **å€™é€‰åº“è¯„ä¼°**ï¼š

   | åº“å | å…¼å®¹æ€§ | ä¼˜åŠ¿ | åŠ£åŠ¿ |
   |------|-------|------|------|
   | `@observablehq/tdigest` | âœ… çº¯ ESM | æ— ä¾èµ–ã€ä½“ç§¯å° | åŠŸèƒ½è¾ƒç®€å• |
   | `tdigest` | âš ï¸ éœ€éªŒè¯ | åŠŸèƒ½å®Œæ•´ã€å‡†ç¡®åº¦é«˜ | å¯èƒ½ä¾èµ– Node Buffer |
   | `hyperloglog` | âš ï¸ éœ€éªŒè¯ | æ ‡å‡†å®ç° | éƒ¨åˆ†åŒ…ä¾èµ– Buffer |
   | WASM æ–¹æ¡ˆï¼ˆRustï¼‰ | âœ… Workers åŸç”Ÿæ”¯æŒ | æ€§èƒ½æœ€ä¼˜ | éœ€è‡ªè¡Œç¼–è¯‘ |

2. **éªŒè¯æ­¥éª¤**ï¼š
   ```ts
   // åœ¨ Miniflare æˆ– wrangler dev ç¯å¢ƒæµ‹è¯•
   import TDigest from '@observablehq/tdigest';
   
   const td = new TDigest();
   for (let i = 0; i < 1000; i++) {
     td.push(Math.random() * 100);
   }
   
   // åºåˆ—åŒ–æµ‹è¯•
   const serialized = td.toJSON(); // æˆ– td.toBytes()
   const restored = TDigest.fromJSON(serialized);
   
   console.log('p95:', restored.percentile(0.95));
   
   // å­˜å‚¨åˆ° D1 æµ‹è¯•
   await env.D1.prepare('INSERT INTO test (data) VALUES (?)')
     .bind(JSON.stringify(serialized))
     .run();
   ```

3. **å…¼å®¹æ€§æ£€æŸ¥é¡¹**ï¼š
   - [ ] å¯¼å…¥æˆåŠŸï¼ˆæ—  `require()` æˆ– Node å†…ç½®æ¨¡å—ï¼‰
   - [ ] åºåˆ—åŒ–/ååºåˆ—åŒ–æ­£å¸¸
   - [ ] æ€§èƒ½è¾¾æ ‡ï¼ˆå¤„ç† 100 æ¡äº‹ä»¶ < 10msï¼‰
   - [ ] å†…å­˜å ç”¨åˆç†ï¼ˆ< 1 MB/å®ä¾‹ï¼‰
   - [ ] BLOB å­˜å‚¨åˆ° D1 åå¯æ­£ç¡®æ¢å¤

**å¤‡é€‰æ–¹æ¡ˆ**ï¼ˆè‹¥éªŒè¯å¤±è´¥ï¼‰ï¼š

1. **ç®€åŒ–ç»Ÿè®¡**ï¼š
   ```ts
   // ä¸ä½¿ç”¨ t-digestï¼Œæ”¹ç”¨æ’åºæ•°ç»„è®¡ç®—ç™¾åˆ†ä½
   const responseTimes = events.map(e => e.responseTime).sort((a, b) => a - b);
   const p95Index = Math.floor(responseTimes.length * 0.95);
   const p95 = responseTimes[p95Index];
   
   // å­˜å‚¨é‡‡æ ·æ•°æ®ï¼ˆæœ€å¤šä¿ç•™ 1000 ä¸ªæ ·æœ¬ï¼‰
   const samples = responseTimes.slice(0, 1000);
   await env.D1.prepare('UPDATE path_stats_hourly SET response_samples = ?')
     .bind(JSON.stringify(samples))
     .run();
   ```

2. **Bloom Filter ä»£æ›¿ HLL**ï¼ˆunique IPï¼‰ï¼š
   - Workers æœ‰çº¯ JS å®ç°çš„ Bloom Filterï¼ˆå¦‚ `bloom-filters`ï¼‰ã€‚
   - å‡†ç¡®åº¦ç•¥ä½ï¼ˆå¯èƒ½é«˜ä¼° 1-2%ï¼‰ï¼Œä½†è¶³å¤Ÿå®ç”¨ã€‚

3. **è‡ªç ” WASM æ–¹æ¡ˆ**ï¼ˆPhase 5ï¼‰ï¼š
   - ä½¿ç”¨ Rust å®ç° t-digest + HLLï¼Œç¼–è¯‘ä¸º WASMã€‚
   - æ‰“åŒ…ä¸º `.wasm` æ¨¡å—éš Worker éƒ¨ç½²ã€‚
   - ä¼˜åŠ¿ï¼šæ€§èƒ½æœ€ä¼˜ã€æ— å…¼å®¹æ€§é—®é¢˜ã€ä½“ç§¯å°ï¼ˆ< 100 KBï¼‰ã€‚

**å†³ç­–æµç¨‹**ï¼š

```
Phase 0 éªŒè¯
    â”‚
    â”œâ”€ Workers å…¼å®¹ â†’ ä½¿ç”¨é€‰å®šçš„ npm åŒ…
    â”‚
    â””â”€ ä¸å…¼å®¹
        â”‚
        â”œâ”€ ç®€åŒ–ç»Ÿè®¡ï¼ˆæ’åºæ•°ç»„ + Bloom Filterï¼‰â†’ Phase 2 å®æ–½
        â”‚
        â””â”€ Phase 4~5 è¿ç§»åˆ° WASM æ–¹æ¡ˆ
```

---

## æ€»ç»“ï¼šå…³é”®ä¿®æ­£ä¸è¡¥å……

| åŸé—®é¢˜ | ä¿®æ­£æ–¹æ¡ˆ | æ¶‰åŠç« èŠ‚ |
|--------|----------|----------|
| D1 ä¸æ”¯æŒ UDF | åœ¨ Worker ä¸­å®Œæˆ t-digest/HLL åˆå¹¶ï¼ŒSQL ä»…åš upsert | 5.2, 5.3 |
| D1 å®¹é‡ä¸è¶³ | æ˜ç»†å¼ºåˆ¶ 3 å¤©ä¿ç•™ï¼Œæ¯æ—¥è‡ªåŠ¨å½’æ¡£è‡³ R2ï¼Œå®¹é‡ç›‘æ§å‘Šè­¦ | 5.3, é™„å½• G |
| Phase 1 æ¶æ„ä½æ•ˆ | Worker ç›´æ¥å†™ Queueï¼Œä¿ç•™æ—§ DO ä½œå…œåº•è¯»è·¯å¾„ | 5.1, 7 |
| åŒå†™é‡å¤è®¡æ•° | å¹‚ç­‰ ID + KV æ»‘åŠ¨çª—å£å»é‡ + å¹¶è¡ŒéªŒè¯ | é™„å½• F |
| **å¹¶å‘å†™å…¥ä¸¢å¢é‡** | **ä½¿ç”¨ DO èšåˆåè°ƒå™¨æˆ–å•æ¶ˆè´¹è€…ä¸²è¡Œå¤„ç†** | **5.3, é™„å½• H Q7** |
| **å½’æ¡£ OOM/è¶…æ—¶** | **åˆ†æ‰¹æŸ¥è¯¢ + R2 Multipart Upload æµå¼å†™å…¥** | **é™„å½• G, é™„å½• H Q8** |
| **Workers 50 subrequest é™åˆ¶ + æ¶ˆæ¯é™é»˜ä¸¢å¤±** | **DO è°ƒç”¨åˆ†å—ï¼ˆâ‰¤45 ä¸ª/æ‰¹ï¼‰ï¼›è·Ÿè¸ªå¤±è´¥ keyï¼Œé€‰æ‹©æ€§ ack/retry æ¶ˆæ¯** | **5.3, é™„å½• H Q9** |
| **R2 å½’æ¡£ OOM + åˆ†ç‰‡å¤§å°é—®é¢˜** | **æ–¹æ¡ˆ Aï¼šçœŸæ­£çš„æµå¼ `put()`ï¼ˆReadableStream pullï¼‰ï¼›æ–¹æ¡ˆ Bï¼šç´¯ç§¯åˆ° 5 MiB å†ä¸Šä¼  part** | **é™„å½• G, é™„å½• H Q10** |
| **D1 DELETE LIMIT ä¸æ”¯æŒ** | **ä½¿ç”¨ rowid å­æŸ¥è¯¢ï¼šDELETE WHERE rowid IN (SELECT ... LIMIT)** | **é™„å½• G, é™„å½• H Q11** |
| **KV åˆ·æ–°è®¡æ•°é€»è¾‘é”™è¯¯** | **å…ˆé€’å¢å†åˆ¤æ–­ï¼šconst next = count + 1; if (next % 10 === 0)** | **5.3, é™„å½• H Q12** |
| **npm åº“å…¼å®¹æ€§é£é™©** | **Phase 0 å‰ç½®éªŒè¯ Workers å…¼å®¹çš„ t-digest/HLL åº“ï¼Œå¤‡é€‰ç®€åŒ–ç»Ÿè®¡** | **9, é™„å½• H Q13** |
| å®æ–½æ¡ä»¶ä¸æ˜ | è¡¥å……å¹‚ç­‰ ID æ€§èƒ½éªŒè¯ã€å½’æ¡£æµç¨‹è®¾è®¡ã€Cron é…ç½® | 9, é™„å½• G |

**å®æ–½é¡ºåºå»ºè®®**ï¼ˆå·²æ›´æ–°ï¼‰ï¼š

1. **Phase 0**ï¼š
   - éªŒè¯å¹‚ç­‰ ID ç”Ÿæˆæ€§èƒ½ã€è®¾è®¡ D1 è¡¨ç»“æ„ã€å‡†å¤‡ R2 å½’æ¡£æµ‹è¯•ã€‚
   - **ç¡®è®¤æ¶ˆè´¹è€…å¹¶å‘ç­–ç•¥**ï¼ˆå•æ¶ˆè´¹è€… vs DO åè°ƒå™¨ï¼‰ã€‚

2. **Phase 1**ï¼š
   - Worker ç›´æ¥å†™ Queueï¼Œå®ç°å»é‡é€»è¾‘ï¼Œä¿ç•™æ—§ DO ä½œå…œåº•ã€‚
   - **é…ç½® `max_concurrency=1`** é¿å…å¹¶å‘å†²çªã€‚

3. **Phase 2**ï¼š
   - å¼€å‘ Aggregator Workerï¼ˆå†…å­˜åˆå¹¶ç»Ÿè®¡ï¼‰+ D1 å†™å…¥ + KV å¿«ç…§ã€‚
   - å®ç°**åˆ†æ‰¹å½’æ¡£ Cron**ï¼ˆMultipart Uploadï¼‰ã€‚
   - é…ç½®é˜Ÿåˆ—ç§¯å‹ç›‘æ§ä¸æ¶ˆè´¹è€…å¿ƒè·³æ£€æµ‹ã€‚

4. **Phase 3**ï¼š
   - åˆ‡æ¢æ¥å£è¯»è·¯å¾„ï¼ˆCacheâ†’KVâ†’D1ï¼‰ï¼Œç°åº¦éªŒè¯åä¸‹çº¿æ—§ DOã€‚
   - å‹åŠ›æµ‹è¯•éªŒè¯å•æ¶ˆè´¹è€…ååé‡ã€‚

5. **Phase 4**ï¼š
   - å®Œå–„ç›‘æ§å‘Šè­¦ï¼ˆé˜Ÿåˆ—ç§¯å‹ã€D1 å®¹é‡ã€èšåˆå»¶è¿Ÿã€å½’æ¡£å¤±è´¥ï¼‰ã€‚
   - **è‹¥æµé‡è¶…è¿‡ 50 ä¸‡/æ—¥ï¼Œè¿ç§»åˆ° DO èšåˆåè°ƒå™¨**ã€‚

6. **Phase 5**ï¼š
   - ä¼˜åŒ–å½’æ¡£æ ¼å¼ï¼ˆParquetï¼‰ã€Analytics Engine é›†æˆã€å¤šåº“åˆ†ç‰‡ã€‚

**å…³é”®å®æ–½æ£€æŸ¥æ¸…å•**ï¼ˆæ–°å¢ï¼‰ï¼š

- [ ] **âš ï¸ Phase 0 å¿…åš**ï¼šéªŒè¯ Workers å…¼å®¹çš„ t-digest/HLL åº“ï¼ˆè§é™„å½• H Q13ï¼‰ã€‚
- [ ] ç¡®è®¤é˜Ÿåˆ—æ¶ˆè´¹è€…å¹¶å‘é…ç½®ï¼ˆ`wrangler.toml` ä¸­ `max_concurrency`ï¼‰ã€‚
- [ ] éªŒè¯ R2 Multipart Upload API åœ¨ Workers ä¸­çš„å…¼å®¹æ€§ã€‚
  - [ ] æµ‹è¯•å•æ¬¡ `put()` æµå¼ä¸Šä¼ ï¼ˆæ¨èï¼Œ<5 GiB åœºæ™¯ï¼‰
  - [ ] è‹¥ä½¿ç”¨ Multipartï¼Œç¡®ä¿åˆ†ç‰‡ â‰¥5 MiBï¼ˆBATCH_SIZE â‰¥50000ï¼‰
- [ ] æµ‹è¯•åˆ†æ‰¹å½’æ¡£æµç¨‹ï¼ˆæ¨¡æ‹Ÿ 100 ä¸‡æ¡è®°å½•ï¼‰ã€‚
- [ ] å®ç°æ¶ˆè´¹è€…å¿ƒè·³ç›‘æ§ï¼ˆå•æ¶ˆè´¹è€…åœºæ™¯å¿…éœ€ï¼‰ã€‚
- [ ] å‡†å¤‡ DO èšåˆåè°ƒå™¨ä»£ç ï¼ˆPhase 4 æ‰©å®¹é¢„æ¡ˆï¼‰ã€‚
  - [ ] éªŒè¯ 50 subrequest é™åˆ¶å¤„ç†ï¼ˆåˆ†å— â‰¤45 ä¸ª/æ‰¹ï¼‰
- [ ] é…ç½®å½’æ¡£å¤±è´¥å‘Šè­¦ï¼ˆR2 ä¸Šä¼ å¤±è´¥ã€D1 åˆ é™¤å¤±è´¥ï¼‰ã€‚
- [ ] éªŒè¯ D1 rowid å­æŸ¥è¯¢åˆ é™¤æ€§èƒ½ï¼ˆåˆ†æ‰¹ 5000 æ¡ï¼‰ã€‚
- [ ] æµ‹è¯• KV åˆ·æ–°è®¡æ•°é€»è¾‘ï¼ˆç¡®ä¿ç¬¬ 10ã€20ã€30... æ¬¡è§¦å‘ï¼‰ã€‚
