# Phase 2 å®æ–½è®¡åˆ’ï¼šAggregator Worker + D1 å­˜å‚¨

## ğŸ“‹ æ¦‚è¿°

**ç›®æ ‡**ï¼šå®ç°å®Œæ•´çš„äº‹ä»¶èšåˆã€æŒä¹…åŒ–å­˜å‚¨ã€å¿«ç…§ç®¡ç†å’Œå½’æ¡£æµç¨‹

**ä¾èµ–**ï¼š
- âœ… Phase 0ï¼šç®€åŒ–ç»Ÿè®¡æ–¹æ¡ˆéªŒè¯é€šè¿‡
- âœ… Phase 1ï¼šWorkers Queue + å¹‚ç­‰ ID + åŸºç¡€æ¶ˆè´¹è€…

**é¢„æœŸæˆæœ**ï¼š
- âœ… äº‹ä»¶æ˜ç»†å’Œèšåˆæ•°æ®æŒä¹…åŒ–åˆ° D1
- âœ… KV å¿«ç…§è‡ªåŠ¨åˆ·æ–°ï¼ˆç‰ˆæœ¬åŒ–ç®¡ç†ï¼‰
- âœ… æ¯æ—¥è‡ªåŠ¨å½’æ¡£è‡³ R2 + D1 æ¸…ç†
- âœ… å•æ¶ˆè´¹è€…å¿ƒè·³ç›‘æ§

---

## ğŸ¯ Phase 2 ä»»åŠ¡æ¸…å•

### Stage 1: æ•°æ®æŒä¹…åŒ–åŸºç¡€ (Tasks 1-3)

#### Task 1: è®¾è®¡å¹¶åˆ›å»º D1 è¡¨ç»“æ„ â³

**è¾“å‡ºæ–‡ä»¶**ï¼š
- `apps/api/migrations/0001_create_path_stats_tables.sql`
- `apps/api/docs/d1-schema.md`ï¼ˆè¡¨ç»“æ„æ–‡æ¡£ï¼‰

**è¡¨è®¾è®¡**ï¼ˆåŸºäºç®€åŒ–ç»Ÿè®¡æ–¹æ¡ˆï¼‰ï¼š

```sql
-- æ˜ç»†äº‹ä»¶è¡¨
CREATE TABLE traffic_events (
  id TEXT PRIMARY KEY,              -- å¹‚ç­‰ ID
  path TEXT NOT NULL,
  method TEXT,
  status INTEGER,
  response_time REAL,
  client_ip_hash TEXT,              -- å·²å“ˆå¸Œçš„ IP
  timestamp INTEGER,
  event_date TEXT,                  -- YYYY-MM-DDï¼ˆç”¨äºåˆ†åŒºï¼‰
  user_agent TEXT,
  country TEXT,
  is_error INTEGER DEFAULT 0        -- 1 = status >= 400
);

CREATE INDEX idx_events_date ON traffic_events(event_date);
CREATE INDEX idx_events_path_date ON traffic_events(path, event_date);
CREATE INDEX idx_events_timestamp ON traffic_events(timestamp);

-- å°æ—¶èšåˆè¡¨ï¼ˆä½¿ç”¨ç®€åŒ–ç»Ÿè®¡ï¼‰
CREATE TABLE path_stats_hourly (
  path TEXT NOT NULL,
  hour_bucket TEXT NOT NULL,        -- '2025-10-15T14'
  requests INTEGER NOT NULL DEFAULT 0,
  errors INTEGER NOT NULL DEFAULT 0,
  sum_response_time REAL NOT NULL DEFAULT 0,
  count_response_time INTEGER NOT NULL DEFAULT 0,
  response_samples TEXT,            -- JSON æ•°ç»„ï¼Œæœ€å¤š 1000 ä¸ªï¼ˆæ°´åº“é‡‡æ ·ï¼‰
  ip_hashes TEXT,                   -- JSON æ•°ç»„ï¼Œæœ€å¤š 1000 ä¸ªï¼ˆæ°´åº“é‡‡æ ·ï¼‰
  unique_ips_seen INTEGER NOT NULL DEFAULT 0,  -- æ°´åº“ä¸­çš„å”¯ä¸€ IP æ•°ï¼ˆä¸‹ç•Œä¼°è®¡ï¼‰
  created_at INTEGER,
  updated_at INTEGER,
  PRIMARY KEY (path, hour_bucket)
);

CREATE INDEX idx_stats_hour ON path_stats_hourly(hour_bucket);
CREATE INDEX idx_stats_updated ON path_stats_hourly(updated_at);

-- å½’æ¡£å…ƒæ•°æ®è¡¨
CREATE TABLE archive_metadata (
  date TEXT PRIMARY KEY,            -- YYYY-MM-DD
  r2_path TEXT NOT NULL,
  record_count INTEGER NOT NULL,
  file_size_bytes INTEGER,
  archived_at INTEGER NOT NULL,
  status TEXT DEFAULT 'completed'   -- completed/failed
);
```

**éªŒè¯æ­¥éª¤**ï¼š
```bash
# åˆ›å»º D1 æ•°æ®åº“ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
wrangler d1 create path-stats-db

# æ‰§è¡Œè¿ç§»
wrangler d1 execute path-stats-db --file=./migrations/0001_create_path_stats_tables.sql

# éªŒè¯è¡¨ç»“æ„
wrangler d1 execute path-stats-db --command="SELECT name FROM sqlite_master WHERE type='table'"
```

---

#### Task 2: å®ç°é˜Ÿåˆ—æ¶ˆè´¹è€…èšåˆé€»è¾‘ â³

**ä¿®æ”¹æ–‡ä»¶**ï¼š`apps/api/src/queue-consumer.ts`

**æ ¸å¿ƒé€»è¾‘**ï¼š

```typescript
import { aggregateEvents, generateStatsSummary } from './lib/simplified-stats';
import type { Env } from './types/env';

interface TrafficEvent {
  idempotentId: string;
  timestamp: number;
  path: string;
  method: string;
  clientIpHash: string;
  userAgent?: string;
  country?: string;
  responseTime?: number;
  isError?: boolean;
}

export default {
  async queue(
    batch: MessageBatch<TrafficEvent>,
    env: Env,
    ctx: ExecutionContext
  ): Promise<void> {
    console.log(`ğŸ“¦ Queue Batch: ${batch.messages.length} messages`);

    // 1. æ›´æ–°å¿ƒè·³
    ctx.waitUntil(updateHeartbeat(env));

    // 2. æŒ‰ (path, hour_bucket) åˆ†ç»„
    const groups = new Map<string, TrafficEvent[]>();
    for (const msg of batch.messages) {
      const event = msg.body;
      const hourBucket = getHourBucket(event.timestamp);
      const key = `${event.path}:${hourBucket}`;
      
      if (!groups.has(key)) {
        groups.set(key, []);
      }
      groups.get(key)!.push(event);
    }

    // 3. é€ç»„å¤„ç†èšåˆ
    for (const [key, events] of groups.entries()) {
      try {
        await processGroup(env, key, events);
      } catch (error) {
        console.error(`âŒ èšåˆå¤±è´¥ [${key}]:`, error);
        // Phase 2: ç»§ç»­å¤„ç†å…¶ä»–ç»„ï¼Œè®°å½•é”™è¯¯
      }
    }

    // 4. å…¨éƒ¨ç¡®è®¤ï¼ˆPhase 2 ç®€åŒ–ç‰ˆï¼ŒPhase 3 å®ç°é€‰æ‹©æ€§ ackï¼‰
    for (const msg of batch.messages) {
      msg.ack();
    }

    console.log(`âœ… Batch processed: ${groups.size} groups`);
  }
};

// è¾…åŠ©å‡½æ•°
function getHourBucket(timestamp: number): string {
  const date = new Date(timestamp);
  return `${date.getUTCFullYear()}-${String(date.getUTCMonth() + 1).padStart(2, '0')}-${String(date.getUTCDate()).padStart(2, '0')}T${String(date.getUTCHours()).padStart(2, '0')}`;
}

async function updateHeartbeat(env: Env) {
  await env.API_GATEWAY_STORAGE.put(
    'aggregator:heartbeat',
    Date.now().toString(),
    { expirationTtl: 300 }
  );
}

async function processGroup(env: Env, key: string, events: TrafficEvent[]) {
  const [path, hourBucket] = key.split(':');
  
  // 1. æ’å…¥æ˜ç»†äº‹ä»¶åˆ° D1
  await insertEvents(env, events);
  
  // 2. è¯»å–ç°æœ‰èšåˆæ•°æ®
  const existing = await getExistingStats(env, path, hourBucket);
  
  // 3. èšåˆç»Ÿè®¡
  const updated = await aggregateEvents(events, existing);
  
  // 4. å†™å› D1
  await upsertStats(env, updated);
  
  console.log(`âœ… Aggregated [${key}]: ${events.length} events`);
}
```

**åˆ›å»ºæ–°æ–‡ä»¶**ï¼š`apps/api/src/lib/queue-aggregator.ts`
- å°è£… `processGroup`ã€`insertEvents`ã€`getExistingStats`ã€`upsertStats` ç­‰å‡½æ•°

---

#### Task 3: å®ç° D1 å†™å…¥é€»è¾‘ â³

**æ–°æ–‡ä»¶**ï¼š`apps/api/src/lib/d1-writer.ts`

**æ ¸å¿ƒå‡½æ•°**ï¼š

```typescript
import type { Env } from '../types/env';
import type { SimplifiedStats } from './simplified-stats';

/**
 * æ‰¹é‡æ’å…¥æ˜ç»†äº‹ä»¶
 */
export async function insertEvents(
  env: Env,
  events: TrafficEvent[]
): Promise<void> {
  if (events.length === 0) return;

  // æ‰¹é‡æ’å…¥ï¼ˆD1 æ”¯æŒäº‹åŠ¡ï¼‰
  const statements = events.map(event => {
    const eventDate = new Date(event.timestamp).toISOString().split('T')[0];
    return env.D1.prepare(
      `INSERT OR IGNORE INTO traffic_events 
       (id, path, method, status, response_time, client_ip_hash, timestamp, event_date, user_agent, country, is_error)
       VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)`
    ).bind(
      event.idempotentId,
      event.path,
      event.method,
      event.status || 200,
      event.responseTime || 0,
      event.clientIpHash,
      event.timestamp,
      eventDate,
      event.userAgent || null,
      event.country || null,
      event.isError ? 1 : 0
    );
  });

  await env.D1.batch(statements);
}

/**
 * è¯»å–ç°æœ‰èšåˆç»Ÿè®¡
 */
export async function getExistingStats(
  env: Env,
  path: string,
  hourBucket: string
): Promise<SimplifiedStats | null> {
  const result = await env.D1.prepare(
    `SELECT * FROM path_stats_hourly WHERE path = ? AND hour_bucket = ?`
  ).bind(path, hourBucket).first();

  if (!result) return null;

  return {
    path: result.path as string,
    hour_bucket: result.hour_bucket as string,
    requests: result.requests as number,
    errors: result.errors as number,
    sum_response_time: result.sum_response_time as number,
    count_response_time: result.count_response_time as number,
    response_samples: JSON.parse(result.response_samples as string || '[]'),
    ip_hashes: JSON.parse(result.ip_hashes as string || '[]'),
    unique_ips_seen: result.unique_ips_seen as number
  };
}

/**
 * Upsert èšåˆç»Ÿè®¡
 */
export async function upsertStats(
  env: Env,
  stats: SimplifiedStats
): Promise<void> {
  await env.D1.prepare(
    `INSERT OR REPLACE INTO path_stats_hourly 
     (path, hour_bucket, requests, errors, sum_response_time, count_response_time, 
      response_samples, ip_hashes, unique_ips_seen, updated_at)
     VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)`
  ).bind(
    stats.path,
    stats.hour_bucket,
    stats.requests,
    stats.errors,
    stats.sum_response_time,
    stats.count_response_time,
    JSON.stringify(stats.response_samples),
    JSON.stringify(stats.ip_hashes),
    stats.unique_ips_seen,
    Date.now()
  ).run();
}
```

---

### Stage 2: å¿«ç…§ç®¡ç† (Task 4)

#### Task 4: å®ç° KV å¿«ç…§ç”Ÿæˆä¸åˆ·æ–° â³

**æ–°æ–‡ä»¶**ï¼š`apps/api/src/lib/kv-snapshot.ts`

**æ ¸å¿ƒé€»è¾‘**ï¼š

```typescript
/**
 * ç”Ÿæˆå¹¶åˆ·æ–° KV å¿«ç…§
 * 
 * ç‰ˆæœ¬åŒ–è®¾è®¡ï¼š
 * - paths:snapshot:{version} - å¿«ç…§å†…å®¹
 * - paths:snapshot:latest - æœ€æ–°ç‰ˆæœ¬æŒ‡é’ˆ
 */
export async function refreshKVSnapshot(env: Env): Promise<void> {
  // 1. ä» D1 è¯»å– Top 100 è·¯å¾„ï¼ˆæŒ‰æœ€è¿‘ 24 å°æ—¶è¯·æ±‚æ•°æ’åºï¼‰
  const cutoffTime = Date.now() - 24 * 3600 * 1000;
  const hourBucket = getHourBucket(cutoffTime);
  
  const { results } = await env.D1.prepare(
    `SELECT path, 
            SUM(requests) as total_requests,
            SUM(errors) as total_errors,
            SUM(sum_response_time) / SUM(count_response_time) as avg_response_time
     FROM path_stats_hourly
     WHERE hour_bucket >= ?
     GROUP BY path
     ORDER BY total_requests DESC
     LIMIT 100`
  ).bind(hourBucket).all();

  // 2. ç”Ÿæˆå¿«ç…§
  const snapshot = {
    version: Date.now(),
    generatedAt: new Date().toISOString(),
    paths: results,
    totalPaths: results?.length || 0
  };

  // 3. è·å–æ—§ç‰ˆæœ¬æŒ‡é’ˆ
  const latestPointer = await env.API_GATEWAY_STORAGE.get(
    'paths:snapshot:latest',
    'json'
  );

  // 4. å†™å…¥æ–°å¿«ç…§
  const newVersion = snapshot.version;
  await env.API_GATEWAY_STORAGE.put(
    `paths:snapshot:${newVersion}`,
    JSON.stringify(snapshot),
    { expirationTtl: 72 * 3600 } // 3 å¤©
  );

  // 5. æ›´æ–°æŒ‡é’ˆ
  await env.API_GATEWAY_STORAGE.put(
    'paths:snapshot:latest',
    JSON.stringify({
      version: newVersion,
      prev: latestPointer?.version || null,
      updatedAt: new Date().toISOString()
    })
  );

  console.log(`âœ… KV snapshot refreshed: version ${newVersion}`);
}
```

**é›†æˆåˆ°èšåˆæµç¨‹**ï¼š
- åœ¨ `queue-consumer.ts` ä¸­ï¼Œæ¯å¤„ç† 10 ä¸ªæ‰¹æ¬¡åˆ·æ–°ä¸€æ¬¡ KV
- æˆ–è€…ä½¿ç”¨ç‹¬ç«‹çš„ Cron Triggerï¼ˆæ¯ 5 åˆ†é’Ÿï¼‰

---

### Stage 3: å½’æ¡£ä¸æ¸…ç† (Tasks 6-8)

#### Task 6: å®ç° R2 å½’æ¡£ Worker â³

**æ–°æ–‡ä»¶**ï¼š`apps/api/src/workers/archive-worker.ts`

**æ ¸å¿ƒé€»è¾‘**ï¼ˆå‚è€ƒæŠ€æœ¯æ–¹æ¡ˆé™„å½• Gï¼‰ï¼š

```typescript
/**
 * æ¯æ—¥å½’æ¡£ Worker
 * 
 * Cron: 0 2 * * * (æ¯æ—¥å‡Œæ™¨ 2 ç‚¹)
 */
export default {
  async scheduled(event: ScheduledEvent, env: Env, ctx: ExecutionContext) {
    await ctx.waitUntil(archiveDailyEvents(env));
  }
};

async function archiveDailyEvents(env: Env) {
  // 1. è®¡ç®—ç›®æ ‡æ—¥æœŸï¼ˆ3 å¤©å‰ï¼‰
  const targetDate = new Date();
  targetDate.setDate(targetDate.getDate() - 3);
  const dateStr = targetDate.toISOString().split('T')[0];

  console.log(`å¼€å§‹å½’æ¡£ ${dateStr} çš„æ•°æ®`);

  // 2. æŸ¥è¯¢è®°å½•æ•°
  const countResult = await env.D1.prepare(
    'SELECT COUNT(*) as count FROM traffic_events WHERE event_date = ?'
  ).bind(dateStr).first<{ count: number }>();

  const totalCount = countResult?.count || 0;
  if (totalCount === 0) {
    console.log(`${dateStr} æ— æ•°æ®ï¼Œè·³è¿‡`);
    return;
  }

  // 3. å†³ç­–ä¸Šä¼ ç­–ç•¥
  const estimatedSizeMB = (totalCount * 150 * 0.25) / (1024 * 1024);
  
  if (estimatedSizeMB < 100) {
    await archiveWithSinglePut(env, dateStr, totalCount);
  } else {
    await archiveWithMultipart(env, dateStr, totalCount);
  }
}

// å®ç° archiveWithSinglePut å’Œ archiveWithMultipart
// ï¼ˆå‚è€ƒæŠ€æœ¯æ–¹æ¡ˆé™„å½• G çš„å®Œæ•´ä»£ç ï¼‰
```

---

#### Task 7: å®ç° D1 æ¸…ç†é€»è¾‘ â³

**é›†æˆåˆ°å½’æ¡£æµç¨‹**ï¼š

```typescript
async function cleanupArchivedData(env: Env, dateStr: string) {
  console.log(`å¼€å§‹æ¸…ç† ${dateStr} çš„ D1 æ•°æ®`);

  let deletedTotal = 0;
  while (true) {
    const result = await env.D1.prepare(`
      DELETE FROM traffic_events 
      WHERE rowid IN (
        SELECT rowid FROM traffic_events 
        WHERE event_date = ? 
        LIMIT 5000
      )
    `).bind(dateStr).run();

    deletedTotal += result.meta.changes || 0;

    if ((result.meta.changes || 0) < 5000) break;

    // çŸ­æš‚ç­‰å¾…ï¼Œé¿å…é•¿æ—¶é—´é”è¡¨
    await new Promise(resolve => setTimeout(resolve, 100));
  }

  console.log(`âœ… å·²æ¸…ç† ${deletedTotal} æ¡è®°å½•`);
}
```

---

#### Task 8: é…ç½® Cron Triggers â³

**ä¿®æ”¹æ–‡ä»¶**ï¼š`apps/api/wrangler.toml`

```toml
# ============================================
# Cron Triggersï¼ˆPhase 2: å½’æ¡£ä¸ç›‘æ§ï¼‰
# ============================================

[triggers]
crons = [
  "0 2 * * *",      # æ¯æ—¥å½’æ¡£ï¼ˆå‡Œæ™¨ 2 ç‚¹ï¼‰
  "0 */6 * * *",    # æ¯ 6 å°æ—¶æ£€æŸ¥ D1 å®¹é‡
  "*/5 * * * *"     # æ¯ 5 åˆ†é’Ÿåˆ·æ–° KV å¿«ç…§ï¼ˆå¯é€‰ï¼Œæˆ–é€šè¿‡æ¶ˆè´¹è€…è§¦å‘ï¼‰
]
```

**åœ¨ `src/index.ts` ä¸­å®ç° `scheduled` handler**ï¼š

```typescript
export default {
  fetch: app.fetch,
  queue: queueConsumer.queue,
  
  async scheduled(event: ScheduledEvent, env: Env, ctx: ExecutionContext) {
    const cron = event.cron;
    
    if (cron === '0 2 * * *') {
      // æ¯æ—¥å½’æ¡£
      await archiveDailyEvents(env);
    } else if (cron === '0 */6 * * *') {
      // å®¹é‡æ£€æŸ¥
      await checkD1Capacity(env);
    } else if (cron === '*/5 * * * *') {
      // KV å¿«ç…§åˆ·æ–°
      await refreshKVSnapshot(env);
    }
  }
} as ExportedHandler<Env>;
```

---

### Stage 4: æµ‹è¯•ä¸æ–‡æ¡£ (Tasks 5, 10-11)

#### Task 5: ç¼–å†™èšåˆé€»è¾‘å•å…ƒæµ‹è¯• â³

**æ–°æ–‡ä»¶**ï¼š`apps/api/tests/unit/queue-aggregator.test.ts`

**æµ‹è¯•åœºæ™¯**ï¼š
- âœ… å•ç»„äº‹ä»¶èšåˆ
- âœ… å¤šç»„å¹¶è¡Œèšåˆ
- âœ… å¢é‡èšåˆï¼ˆå·²æœ‰æ•°æ® + æ–°äº‹ä»¶ï¼‰
- âœ… æ°´åº“é‡‡æ ·æ­£ç¡®æ€§
- âœ… å¹‚ç­‰ ID å»é‡

---

#### Task 10: æœ¬åœ°æµ‹è¯•å®Œæ•´æµç¨‹ â³

**æµ‹è¯•æ­¥éª¤**ï¼š

```bash
# 1. å¯åŠ¨æœ¬åœ°å¼€å‘ç¯å¢ƒ
npm run dev

# 2. å‘é€æµ‹è¯•è¯·æ±‚
for i in {1..100}; do
  curl http://localhost:8787/api/health
done

# 3. æ£€æŸ¥ D1 æ•°æ®
wrangler d1 execute path-stats-db --command="SELECT COUNT(*) FROM traffic_events"
wrangler d1 execute path-stats-db --command="SELECT * FROM path_stats_hourly LIMIT 5"

# 4. æ£€æŸ¥ KV å¿«ç…§
wrangler kv:key get --namespace-id=xxx "paths:snapshot:latest"

# 5. æµ‹è¯•å½’æ¡£æµç¨‹ï¼ˆæ‰‹åŠ¨è§¦å‘ï¼‰
curl -X POST http://localhost:8787/admin/archive/trigger \
  -H "Content-Type: application/json" \
  -d '{"date": "2025-10-12"}'
```

---

#### Task 11: ç¼–å†™ Phase 2 å®ŒæˆæŠ¥å‘Š â³

**è¾“å‡ºæ–‡ä»¶**ï¼š`docs/phase2-completion-report.md`

**å†…å®¹ç»“æ„**ï¼š
- âœ… å®Œæˆçš„åŠŸèƒ½æ¸…å•
- âœ… æµ‹è¯•ç»“æœæ€»ç»“
- âœ… æ€§èƒ½æŒ‡æ ‡ï¼ˆèšåˆè€—æ—¶ã€D1 å†™å…¥é€Ÿåº¦ï¼‰
- âœ… å·²çŸ¥é—®é¢˜ä¸é™åˆ¶
- âœ… Phase 3 å‡†å¤‡äº‹é¡¹

---

## ğŸ“Š éªŒæ”¶æ ‡å‡†

### åŠŸèƒ½éªŒæ”¶

- [ ] D1 è¡¨ç»“æ„åˆ›å»ºæˆåŠŸï¼Œç´¢å¼•æ­£å¸¸
- [ ] é˜Ÿåˆ—æ¶ˆè´¹è€…èƒ½æ­£ç¡®èšåˆäº‹ä»¶å¹¶å†™å…¥ D1
- [ ] æ˜ç»†äº‹ä»¶å’Œèšåˆæ•°æ®æ­£ç¡®å­˜å‚¨
- [ ] KV å¿«ç…§è‡ªåŠ¨åˆ·æ–°ï¼Œç‰ˆæœ¬åŒ–ç®¡ç†æ­£å¸¸
- [ ] R2 å½’æ¡£æµç¨‹èƒ½å®Œæ•´æ‰§è¡Œï¼ˆåŒ…æ‹¬æ¸…ç†ï¼‰
- [ ] Cron Triggers æ­£å¸¸è§¦å‘

### æ€§èƒ½éªŒæ”¶

- [ ] å•æ‰¹æ¬¡ï¼ˆ100 æ¡æ¶ˆæ¯ï¼‰èšåˆè€—æ—¶ < 500ms
- [ ] D1 æ‰¹é‡æ’å…¥ 100 æ¡è®°å½• < 200ms
- [ ] KV å¿«ç…§ç”Ÿæˆ < 1s
- [ ] å½’æ¡£ 10 ä¸‡æ¡è®°å½• < 2 åˆ†é’Ÿ

### æ•°æ®ä¸€è‡´æ€§éªŒæ”¶

- [ ] å¹‚ç­‰ ID æ­£ç¡®ç”Ÿæˆï¼Œæ— é‡å¤
- [ ] èšåˆè®¡æ•°å‡†ç¡®ï¼ˆä¸æ˜ç»†è¡¨å¯¹æ¯”ï¼‰
- [ ] æ°´åº“é‡‡æ ·ç»Ÿè®¡è¯¯å·® < 5%ï¼ˆå¯¹äº <1000 è¯·æ±‚åº”ä¸º 0%ï¼‰
- [ ] å½’æ¡£å D1 æ•°æ®æ­£ç¡®æ¸…ç†

---

## ğŸš€ å¼€å§‹æ‰§è¡Œ

**å½“å‰ä»»åŠ¡**ï¼šTask 1 - è®¾è®¡å¹¶åˆ›å»º D1 è¡¨ç»“æ„

**é¢„è®¡è€—æ—¶**ï¼šPhase 2 å…¨éƒ¨å®Œæˆçº¦ 2-3 å¤©å¼€å‘æ—¶é—´

---

## ğŸ“ ä¿®è®¢å†å²

| æ—¥æœŸ | ç‰ˆæœ¬ | ä¿®æ”¹å†…å®¹ |
|------|------|----------|
| 2025-10-15 | v1.0 | åˆå§‹ç‰ˆæœ¬ï¼ŒPhase 2 å®æ–½è®¡åˆ’ |

